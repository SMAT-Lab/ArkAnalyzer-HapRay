#!/usr/bin/env python3
"""
ä½¿ç”¨ Poe API (OpenAI å…¼å®¹) æ¥åˆ†æåæ±‡ç¼–ä»£ç å¹¶æ¨æ–­å‡½æ•°åŠŸèƒ½å’Œå‡½æ•°å
å‚è€ƒ: https://creator.poe.com/docs/external-applications/tool-calling
"""

import hashlib
import json
import re
import signal
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Optional

from core.utils.config import config
from core.utils.logger import get_logger

logger = get_logger(__name__)

try:
    import openai
except ImportError:
    openai = None
    logger.warning('openai library not installed, LLM analysis will be unavailable')
    logger.warning('Please install openai library: pip install openai')
except Exception as e:
    logger.error('Error loading openai library: %s', e)
    raise


class LLMFunctionAnalyzer:
    """ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹åˆ†æåæ±‡ç¼–ä»£ç """

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = None,
        base_url: Optional[str] = None,
        enable_cache: bool = True,
        save_prompts: bool = False,
        output_dir: Optional[str] = None,
        open_source_lib: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ– LLM åˆ†æå™¨

        Args:
            api_key: API keyï¼Œå¦‚æœä¸º None åˆ™ä»ç¯å¢ƒå˜é‡æˆ– .env æ–‡ä»¶è¯»å–
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œå¦‚æœä¸º None åˆ™ä» config.get_llm_config() è·å–
            base_url: API åŸºç¡€ URLï¼Œå¦‚æœä¸º None åˆ™ä» config.get_llm_config() è·å–
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜ï¼ˆé¿å…é‡å¤åˆ†æç›¸åŒä»£ç ï¼‰
            save_prompts: æ˜¯å¦ä¿å­˜ç”Ÿæˆçš„ prompt åˆ°æ–‡ä»¶
            output_dir: è¾“å‡ºç›®å½•ï¼Œç”¨äºä¿å­˜ prompt æ–‡ä»¶
            open_source_lib: å¼€æºåº“åç§°ï¼ˆå¯é€‰ï¼Œå¦‚ "ffmpeg", "openssl" ç­‰ï¼‰ã€‚å¦‚æœæŒ‡å®šï¼Œä¼šå‘Šè¯‰å¤§æ¨¡å‹è¿™æ˜¯åŸºäºè¯¥å¼€æºåº“çš„å®šåˆ¶ç‰ˆæœ¬
        """
        if openai is None:
            raise ImportError('éœ€è¦å®‰è£… openai åº“: pip install openai')

        # ä»é…ç½®è·å–é»˜è®¤å€¼
        llm_config = config.get_llm_config()
        default_base_url = llm_config['base_url']
        default_model = llm_config['model']

        self.api_key = api_key or llm_config['api_key']
        self.model = model or default_model
        self.base_url = base_url or default_base_url
        self.enable_cache = enable_cache
        self.save_prompts = save_prompts
        self.open_source_lib = open_source_lib  # å¼€æºåº“åç§°

        self.output_dir = Path(output_dir) if output_dir else config.get_output_dir()
        self.cache: dict[str, dict[str, Any]] = {}
        self.cache_file = llm_config['cache_file']

        # è®¾ç½® prompt ä¿å­˜ç›®å½•
        if self.save_prompts:
            self.prompt_output_dir = self.output_dir / 'prompts'
            self.prompt_output_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f'Prompt output directory: {self.prompt_output_dir.absolute()}')

        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        self.cache_file.parent.mkdir(parents=True, exist_ok=True)

        # Token ç»Ÿè®¡
        self.token_stats = {
            'total_requests': 0,
            'total_input_tokens': 0,
            'total_output_tokens': 0,
            'total_tokens': 0,
            'cached_requests': 0,
            'requests': [],  # æ¯æ¬¡è¯·æ±‚çš„è¯¦ç»†ä¿¡æ¯
        }
        self.token_stats_file = llm_config['token_stats_file']

        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        self.client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
        )

        # åŠ è½½ç¼“å­˜
        if self.enable_cache and self.cache_file.exists():
            try:
                with open(self.cache_file, encoding='utf-8') as f:
                    self.cache = json.load(f)
            except Exception:
                # é™é»˜å¤„ç†ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œé¿å…åœ¨å¯¼å…¥æ—¶æ‰“å°
                pass

        # åŠ è½½ token ç»Ÿè®¡
        if self.token_stats_file.exists():
            try:
                with open(self.token_stats_file, encoding='utf-8') as f:
                    saved_stats = json.load(f)
                    # åˆå¹¶ç»Ÿè®¡ï¼ˆä¿ç•™å†å²æ•°æ®ï¼‰
                    self.token_stats['total_requests'] = saved_stats.get('total_requests', 0)
                    self.token_stats['total_input_tokens'] = saved_stats.get('total_input_tokens', 0)
                    self.token_stats['total_output_tokens'] = saved_stats.get('total_output_tokens', 0)
                    self.token_stats['total_tokens'] = saved_stats.get('total_tokens', 0)
                    self.token_stats['cached_requests'] = saved_stats.get('cached_requests', 0)
                    # ä¿ç•™æœ€è¿‘çš„è¯·æ±‚è®°å½•ï¼ˆæœ€å¤š1000æ¡ï¼‰
                    self.token_stats['requests'] = saved_stats.get('requests', [])[-1000:]
            except Exception:
                # é™é»˜å¤„ç†ç»Ÿè®¡åŠ è½½å¤±è´¥ï¼Œé¿å…åœ¨å¯¼å…¥æ—¶æ‰“å°
                pass

    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶"""
        if self.enable_cache:
            try:
                with open(self.cache_file, 'w', encoding='utf-8') as f:
                    json.dump(self.cache, f, ensure_ascii=False, indent=2)
            except Exception:
                # é™é»˜å¤„ç†ç¼“å­˜ä¿å­˜å¤±è´¥
                pass

    def _save_token_stats(self):
        """ä¿å­˜ token ç»Ÿè®¡åˆ°æ–‡ä»¶"""
        try:
            with open(self.token_stats_file, 'w', encoding='utf-8') as f:
                json.dump(self.token_stats, f, ensure_ascii=False, indent=2)
        except Exception:
            # é™é»˜å¤„ç†ç»Ÿè®¡ä¿å­˜å¤±è´¥
            pass

    def get_token_stats(self) -> dict[str, Any]:
        """è·å– token ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_requests': self.token_stats['total_requests'],
            'total_input_tokens': self.token_stats['total_input_tokens'],
            'total_output_tokens': self.token_stats['total_output_tokens'],
            'total_tokens': self.token_stats['total_tokens'],
            'cached_requests': self.token_stats['cached_requests'],
            'average_input_tokens': (
                self.token_stats['total_input_tokens']
                / (self.token_stats['total_requests'] - self.token_stats['cached_requests'])
                if (self.token_stats['total_requests'] - self.token_stats['cached_requests']) > 0
                else 0
            ),
            'average_output_tokens': (
                self.token_stats['total_output_tokens']
                / (self.token_stats['total_requests'] - self.token_stats['cached_requests'])
                if (self.token_stats['total_requests'] - self.token_stats['cached_requests']) > 0
                else 0
            ),
            'recent_requests': self.token_stats['requests'][-10:],  # æœ€è¿‘10æ¬¡è¯·æ±‚
        }

    def print_token_stats(self):
        """æ‰“å° token ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_token_stats()
        logger.info('\n' + '=' * 80)
        logger.info('Token Usage Statistics')
        logger.info('=' * 80)
        logger.info(f'Total requests: {stats["total_requests"]}')
        logger.info(f'Cache hits: {stats["cached_requests"]}')
        logger.info(f'Actual API calls: {stats["total_requests"] - stats["cached_requests"]}')
        logger.info(f'\nTotal tokens: {stats["total_tokens"]:,}')
        logger.info(f'  Input tokens: {stats["total_input_tokens"]:,}')
        logger.info(f'  Output tokens: {stats["total_output_tokens"]:,}')
        if stats['total_requests'] - stats['cached_requests'] > 0:
            logger.info('\nAverage per call:')
            logger.info(f'  Input tokens: {stats["average_input_tokens"]:.0f}')
            logger.info(f'  Output tokens: {stats["average_output_tokens"]:.0f}')
        logger.info('=' * 80)

    def _get_cache_key(
        self,
        instructions: list[str],
        strings: list[str],
        symbol_name: Optional[str] = None,
        called_functions: Optional[list[str]] = None,
        decompiled: Optional[str] = None,
    ) -> str:
        """ç”Ÿæˆç¼“å­˜é”®ï¼ˆä½¿ç”¨ hash æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼‰"""
        # æ„å»ºç”¨äº hash çš„å†…å®¹
        # 1. æŒ‡ä»¤ï¼šä½¿ç”¨å‰50æ¡æŒ‡ä»¤ï¼ˆå¢åŠ æ•°é‡ä»¥æé«˜å”¯ä¸€æ€§ï¼‰+ æŒ‡ä»¤æ€»æ•°
        inst_content = '; '.join(instructions[:50]) if instructions else ''
        inst_count = str(len(instructions))

        # 2. å­—ç¬¦ä¸²ï¼šæ‰€æœ‰å­—ç¬¦ä¸²ï¼ˆæ’åºä»¥ç¡®ä¿ä¸€è‡´æ€§ï¼‰
        strings_sorted = sorted(set(str(s) for s in strings)) if strings else []
        strings_content = ', '.join(strings_sorted)

        # 3. è°ƒç”¨çš„å‡½æ•°ï¼šæ’åºåçš„å‡½æ•°åˆ—è¡¨
        called_sorted = sorted(set(str(f) for f in called_functions)) if called_functions else []
        called_content = ', '.join(called_sorted)

        # 4. åç¼–è¯‘ä»£ç ï¼šå¦‚æœæœ‰ï¼Œä½¿ç”¨å‰500å­—ç¬¦çš„ hashï¼ˆé¿å…è¿‡é•¿ï¼‰
        decompiled_hash = ''
        if decompiled:
            decompiled_preview = decompiled[:500]  # å‰500å­—ç¬¦
            decompiled_hash = hashlib.md5(decompiled_preview.encode('utf-8')).hexdigest()[:16]

        # 5. ç¬¦å·å
        symbol_str = str(symbol_name) if symbol_name else ''

        # ç»„åˆæ‰€æœ‰å†…å®¹å¹¶ç”Ÿæˆ hash
        key_content = '|'.join(
            [
                inst_content,
                inst_count,
                strings_content,
                called_content,
                decompiled_hash,
                symbol_str,
            ]
        )

        # ä½¿ç”¨ MD5 hash ç”Ÿæˆå›ºå®šé•¿åº¦çš„ keyï¼ˆé¿å… key è¿‡é•¿ï¼‰
        return hashlib.md5(key_content.encode('utf-8')).hexdigest()

    def analyze_with_llm(
        self,
        instructions: list[str],
        strings: list[str],
        symbol_name: Optional[str] = None,
        called_functions: Optional[list[str]] = None,
        offset: Optional[int] = None,
        context: Optional[str] = None,
        func_info: Optional[dict] = None,
        call_count: Optional[int] = None,
        event_count: Optional[int] = None,
        so_file: Optional[str] = None,
    ) -> dict[str, Any]:
        """
        ä½¿ç”¨ LLM åˆ†æåæ±‡ç¼–ä»£ç å¹¶æ¨æ–­å‡½æ•°åŠŸèƒ½å’Œå‡½æ•°å

        Args:
            instructions: åæ±‡ç¼–æŒ‡ä»¤åˆ—è¡¨ï¼Œæ ¼å¼: ["0x3291b84: bti c", ...]
            strings: é™„è¿‘çš„å­—ç¬¦ä¸²å¸¸é‡åˆ—è¡¨
            symbol_name: ç¬¦å·è¡¨ä¸­çš„å‡½æ•°åï¼ˆå¦‚æœæœ‰ï¼‰
            called_functions: è°ƒç”¨çš„å‡½æ•°åˆ—è¡¨
            offset: å‡½æ•°åç§»é‡ï¼ˆç”¨äºä¸Šä¸‹æ–‡ï¼‰

        Returns:
            {
                'functionality': 'åŠŸèƒ½æè¿°',
                'function_name': 'æ¨æ–­çš„å‡½æ•°å',
                'confidence': 'é«˜/ä¸­/ä½',
                'reasoning': 'æ¨ç†è¿‡ç¨‹'
            }
        """
        # æ£€æŸ¥ç¼“å­˜
        if self.enable_cache:
            cache_key = self._get_cache_key(instructions, strings, symbol_name, called_functions, None)
            if cache_key in self.cache:
                cached_entry = self.cache[cache_key]
                # å¤„ç†æ–°æ ¼å¼ï¼ˆåŒ…å«å…ƒä¿¡æ¯ï¼‰å’Œæ—§æ ¼å¼ï¼ˆç›´æ¥æ˜¯åˆ†æç»“æœï¼‰
                if isinstance(cached_entry, dict) and 'analysis' in cached_entry:
                    cached_result = cached_entry['analysis']
                else:
                    cached_result = cached_entry

                # ç¡®ä¿æ—§ç¼“å­˜æ ¼å¼ä¹Ÿæœ‰ performance_analysis å­—æ®µ
                if 'performance_analysis' not in cached_result:
                    cached_result['performance_analysis'] = ''
                    logger.debug(
                        'âš ï¸  Cached result missing performance_analysis field, added empty value (recommend clearing cache and re-analyzing)'
                    )

                self.token_stats['cached_requests'] += 1
                self.token_stats['total_requests'] += 1
                logger.debug(f'Using cached result (total instructions: {len(instructions)})')
                return cached_result

        # æ„å»ºæç¤ºè¯
        logger.debug(f'Building new prompt (total instructions: {len(instructions)})')
        prompt = self._build_prompt(
            instructions,
            strings,
            symbol_name,
            called_functions,
            offset,
            context,
            func_info=func_info,
            call_count=call_count,
            event_count=event_count,
            so_file=so_file,
            open_source_lib=self.open_source_lib,
        )

        # ä¿å­˜ promptï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.save_prompts:
            self._save_single_prompt(prompt, offset, symbol_name)

        try:
            # è°ƒç”¨ LLMï¼ˆè®¾ç½®è¶…æ—¶é¿å…å¡ä½ï¼‰
            # ä»é…ç½®è·å–è¶…æ—¶æ—¶é—´
            llm_config = config.get_llm_config()
            timeout_seconds = llm_config['timeout']

            def timeout_handler(signum, frame):
                raise TimeoutError(f'LLM API è°ƒç”¨è¶…æ—¶ï¼ˆè¶…è¿‡ {timeout_seconds} ç§’ï¼‰')

            # ä½¿ç”¨ä¿¡å·è®¾ç½®è¶…æ—¶ï¼ˆä»…é™ Unix ç³»ç»Ÿï¼‰
            if hasattr(signal, 'SIGALRM'):
                old_handler = signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(timeout_seconds)

            try:
                # è°ƒç”¨ LLM
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {
                            'role': 'system',
                            'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é€†å‘å·¥ç¨‹ä¸“å®¶ï¼Œæ“…é•¿åˆ†æ ARM64 æ±‡ç¼–ä»£ç å¹¶æ¨æ–­å‡½æ•°åŠŸèƒ½å’Œå‡½æ•°åã€‚',
                        },
                        {'role': 'user', 'content': prompt},
                    ],
                    temperature=0.0,  # æœ€ä½éšæœºæ€§ï¼Œæœ€é«˜ä¸€è‡´æ€§å’Œç¨³å®šæ€§
                    max_tokens=2000,  # å¢åŠ  token é™åˆ¶ï¼Œç¡®ä¿åŠŸèƒ½æè¿°å’Œæ¨ç†è¿‡ç¨‹å®Œæ•´
                    timeout=timeout_seconds,  # è®¾ç½®è¶…æ—¶
                )

                # ç»Ÿè®¡ token ä½¿ç”¨é‡
                usage = response.usage
                if usage:
                    input_tokens = usage.prompt_tokens if hasattr(usage, 'prompt_tokens') else 0
                    output_tokens = usage.completion_tokens if hasattr(usage, 'completion_tokens') else 0
                    total_tokens = (
                        usage.total_tokens if hasattr(usage, 'total_tokens') else (input_tokens + output_tokens)
                    )

                    # æ›´æ–°ç»Ÿè®¡
                    self.token_stats['total_requests'] += 1
                    self.token_stats['total_input_tokens'] += input_tokens
                    self.token_stats['total_output_tokens'] += output_tokens
                    self.token_stats['total_tokens'] += total_tokens

                    # è®°å½•æœ¬æ¬¡è¯·æ±‚è¯¦æƒ…
                    request_info = {
                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                        'offset': f'0x{offset:x}' if offset else None,
                        'symbol_name': symbol_name,
                        'input_tokens': input_tokens,
                        'output_tokens': output_tokens,
                        'total_tokens': total_tokens,
                        'instruction_count': len(instructions),
                    }
                    self.token_stats['requests'].append(request_info)

                    # åªä¿ç•™æœ€è¿‘1000æ¡è®°å½•
                    if len(self.token_stats['requests']) > 1000:
                        self.token_stats['requests'] = self.token_stats['requests'][-1000:]

                    # å»¶è¿Ÿä¿å­˜ç»Ÿè®¡ï¼ˆæ¯10æ¬¡è¯·æ±‚ä¿å­˜ä¸€æ¬¡ï¼Œå‡å°‘ I/Oï¼‰
                    if self.token_stats['total_requests'] % 10 == 0:
                        self._save_token_stats()
            finally:
                # æ¢å¤ä¿¡å·å¤„ç†
                if hasattr(signal, 'SIGALRM'):
                    signal.alarm(0)
                    signal.signal(signal.SIGALRM, old_handler)

            result_text = response.choices[0].message.content
            logger.debug(f'LLM response: {result_text}')

            # è§£æç»“æœ
            result = self._parse_llm_response(result_text)

            # ä¿å­˜åˆ°ç¼“å­˜ï¼ˆåŒ…å«å…ƒä¿¡æ¯ï¼‰
            if self.enable_cache:
                # å­˜å‚¨åˆ†æç»“æœå’Œå…ƒä¿¡æ¯
                cache_entry = {
                    'analysis': result,  # LLM åˆ†æç»“æœ
                    'metadata': {
                        'instruction_count': len(instructions),
                        'string_count': len(strings),
                        'has_decompiled': False,  # å•å‡½æ•°æ¨¡å¼æš‚ä¸æ”¯æŒåç¼–è¯‘
                        'called_functions_count': len(called_functions) if called_functions else 0,
                        'offset': f'0x{offset:x}' if offset else None,
                    },
                }
                self.cache[cache_key] = cache_entry
                # å»¶è¿Ÿä¿å­˜ç¼“å­˜ï¼ˆæ¯10æ¬¡ä¿å­˜ä¸€æ¬¡ï¼Œå‡å°‘ I/Oï¼‰
                if len(self.cache) % 10 == 0:
                    self._save_cache()

            return result

        except Exception as e:
            logger.warning('Warning: LLM analysis failed: %s', e)
            # è¿”å›é»˜è®¤ç»“æœ
            return {
                'functionality': 'æœªçŸ¥',
                'function_name': None,
                'confidence': 'ä½',
                'reasoning': f'LLM åˆ†æå¤±è´¥: {str(e)}',
            }

    def _build_prompt(
        self,
        instructions: list[str],
        strings: list[str],
        symbol_name: Optional[str],
        called_functions: Optional[list[str]],
        offset: Optional[int],
        context: Optional[str] = None,
        func_info: Optional[dict] = None,
        call_count: Optional[int] = None,
        event_count: Optional[int] = None,
        so_file: Optional[str] = None,
        open_source_lib: Optional[str] = None,
    ) -> str:
        """æ„å»º LLM æç¤ºè¯"""
        prompt_parts = []

        prompt_parts.append('è¯·åˆ†æä»¥ä¸‹ ARM64 åæ±‡ç¼–ä»£ç ï¼Œæ¨æ–­å‡½æ•°çš„åŠŸèƒ½å’Œå¯èƒ½çš„å‡½æ•°åã€‚')
        prompt_parts.append('')

        # æ·»åŠ å¼€æºåº“ç›¸å…³çš„ promptï¼ˆå¦‚æœæŒ‡å®šäº†å¼€æºåº“ï¼‰
        self._add_open_source_lib_prompt(prompt_parts, open_source_lib)

        prompt_parts.append('âš ï¸ é‡è¦æç¤ºï¼šè¿™æ˜¯ä¸€ä¸ªæ€§èƒ½åˆ†æåœºæ™¯ï¼Œè¯¥å‡½æ•°è¢«è¯†åˆ«ä¸ºé«˜æŒ‡ä»¤æ•°è´Ÿè½½çš„çƒ­ç‚¹å‡½æ•°ã€‚')
        prompt_parts.append('è¯·é‡ç‚¹å…³æ³¨å¯èƒ½å¯¼è‡´æ€§èƒ½é—®é¢˜çš„å› ç´ ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š')
        prompt_parts.append('  - å¾ªç¯å’Œè¿­ä»£ï¼ˆç‰¹åˆ«æ˜¯åµŒå¥—å¾ªç¯ã€å¤§å¾ªç¯æ¬¡æ•°ï¼‰')
        prompt_parts.append('  - å†…å­˜æ“ä½œï¼ˆå¤§é‡å†…å­˜æ‹·è´ã€é¢‘ç¹çš„å†…å­˜åˆ†é…/é‡Šæ”¾ï¼‰')
        prompt_parts.append('  - å­—ç¬¦ä¸²å¤„ç†ï¼ˆå­—ç¬¦ä¸²æ‹¼æ¥ã€è§£æã€æ ¼å¼åŒ–ï¼‰')
        prompt_parts.append('  - ç®—æ³•å¤æ‚åº¦ï¼ˆO(nÂ²)ã€O(nÂ³) ç­‰é«˜å¤æ‚åº¦ç®—æ³•ï¼‰')
        prompt_parts.append('  - ç³»ç»Ÿè°ƒç”¨å’Œ I/O æ“ä½œï¼ˆæ–‡ä»¶è¯»å†™ã€ç½‘ç»œæ“ä½œï¼‰')
        prompt_parts.append('  - é€’å½’è°ƒç”¨ï¼ˆæ·±åº¦é€’å½’å¯èƒ½å¯¼è‡´æ ˆæº¢å‡ºæˆ–é«˜æŒ‡ä»¤æ•°ï¼‰')
        prompt_parts.append('  - å¼‚å¸¸å¤„ç†ï¼ˆé¢‘ç¹çš„å¼‚å¸¸æ•è·å’Œå¤„ç†ï¼‰')
        prompt_parts.append('  - é”å’ŒåŒæ­¥æ“ä½œï¼ˆé¢‘ç¹çš„åŠ é”/è§£é”ã€æ¡ä»¶ç­‰å¾…ï¼‰')
        prompt_parts.append('  - æ•°æ®ç»“æ„å’Œç®—æ³•é€‰æ‹©ä¸å½“ï¼ˆä½æ•ˆçš„æ•°æ®ç»“æ„ä½¿ç”¨ï¼‰')
        prompt_parts.append('')
        prompt_parts.append('è¯·åˆ†åˆ«æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š')
        prompt_parts.append('  1. åŠŸèƒ½æè¿°ï¼šå‡½æ•°çš„ä¸»è¦åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼ˆä¸è¦åŒ…å«æ€§èƒ½åˆ†æï¼‰')
        prompt_parts.append('  2. è´Ÿè½½é—®é¢˜è¯†åˆ«ä¸ä¼˜åŒ–å»ºè®®ï¼š')
        prompt_parts.append('     - æ˜¯å¦å­˜åœ¨æ˜æ˜¾çš„æ€§èƒ½ç“¶é¢ˆï¼ˆå¦‚ä¸Šè¿°å› ç´ ï¼‰')
        prompt_parts.append('     - ä¸ºä»€ä¹ˆè¿™ä¸ªå‡½æ•°å¯èƒ½å¯¼è‡´é«˜æŒ‡ä»¤æ•°è´Ÿè½½')
        prompt_parts.append('     - å¯èƒ½çš„ä¼˜åŒ–å»ºè®®ï¼ˆå¦‚æœæœ‰ï¼‰')
        prompt_parts.append('')

        # æ·»åŠ èƒŒæ™¯ä¿¡æ¯
        if context:
            prompt_parts.append('èƒŒæ™¯ä¿¡æ¯:')
            prompt_parts.append(context)
            prompt_parts.append('')

        # å‡½æ•°åŸºæœ¬ä¿¡æ¯
        if offset:
            prompt_parts.append(f'å‡½æ•°åç§»é‡: 0x{offset:x}')

        # å‡½æ•°è¾¹ç•Œä¿¡æ¯
        if func_info:
            func_start = func_info.get('minbound', func_info.get('offset', offset))
            func_end = func_info.get('maxbound', func_start + func_info.get('size', 0))
            func_size = func_info.get('size', 0)
            if func_size > 0:
                prompt_parts.append(f'å‡½æ•°èŒƒå›´: 0x{func_start:x} - 0x{func_end:x} (å¤§å°: {func_size} å­—èŠ‚)')

        # æŒ‡ä»¤æ•°é‡
        if instructions:
            prompt_parts.append(f'æŒ‡ä»¤æ•°é‡: {len(instructions)} æ¡')

        # å‡½æ•°å¤æ‚åº¦æŒ‡æ ‡
        if func_info:
            nbbs = func_info.get('nbbs', 0)
            edges = func_info.get('edges', 0)
            nargs = func_info.get('nargs', 0)
            nlocals = func_info.get('nlocals', 0)
            if nbbs > 0:
                prompt_parts.append(f'åŸºæœ¬å—æ•°é‡: {nbbs}')
            if edges > 0:
                prompt_parts.append(f'æ§åˆ¶æµè¾¹æ•°é‡: {edges}')
            if nargs > 0:
                prompt_parts.append(f'å‚æ•°æ•°é‡: {nargs}')
            if nlocals > 0:
                prompt_parts.append(f'å±€éƒ¨å˜é‡æ•°é‡: {nlocals}')

        # æ³¨æ„ï¼šè°ƒç”¨æ¬¡æ•°ï¼ˆcall_countï¼‰å’ŒæŒ‡ä»¤æ‰§è¡Œæ¬¡æ•°ï¼ˆevent_countï¼‰ä»…ç”¨äºæ’åºå’Œç­›é€‰ï¼Œ
        # ä¸éœ€è¦ä¼ é€’ç»™ LLMï¼Œå› æ­¤ä¸æ·»åŠ åˆ° prompt ä¸­

        # SO æ–‡ä»¶ä¿¡æ¯
        if so_file:
            so_name = so_file.split('/')[-1] if '/' in so_file else so_file
            prompt_parts.append(f'æ‰€åœ¨æ–‡ä»¶: {so_name}')

        if symbol_name:
            prompt_parts.append(f'ç¬¦å·è¡¨ä¸­çš„å‡½æ•°å: {symbol_name}')
            prompt_parts.append('ï¼ˆå¦‚æœç¬¦å·åæ˜¯ C++ åç§°ä¿®é¥°ï¼Œè¯·å°è¯•è¿˜åŸåŸå§‹å‡½æ•°åï¼‰')

        prompt_parts.append('')
        prompt_parts.append('åæ±‡ç¼–ä»£ç :')
        # æ ¹æ®æŒ‡ä»¤æ•°é‡è‡ªé€‚åº”è°ƒæ•´ï¼šçŸ­å‡½æ•°å‘é€å…¨éƒ¨ï¼Œé•¿å‡½æ•°å‘é€å‰300æ¡
        # æ³¨æ„ï¼šç”±äºå‡½æ•°åˆ‡åˆ†é€»è¾‘å·²æ”¹è¿›ï¼Œç°åœ¨èƒ½è·å–æ›´å¤šæŒ‡ä»¤ï¼ˆæœ€å¤šçº¦500æ¡ï¼‰ï¼Œ
        # å› æ­¤å¢åŠ å‘é€ç»™ LLM çš„æŒ‡ä»¤æ•°é‡ï¼Œä»¥æä¾›æ›´å®Œæ•´çš„ä¸Šä¸‹æ–‡
        max_instructions = 300 if len(instructions) > 100 else len(instructions)
        # è°ƒè¯•ä¿¡æ¯ï¼šè®°å½•å®é™…å‘é€çš„æŒ‡ä»¤æ•°
        logger.debug(f'Total instructions: {len(instructions)}, will send to LLM: {max_instructions}')
        for i, inst in enumerate(instructions[:max_instructions], 1):
            prompt_parts.append(f'  {i:3d}. {inst}')

        # å¦‚æœå‡½æ•°å¾ˆé•¿ï¼Œæç¤º LLM
        if len(instructions) > max_instructions:
            prompt_parts.append(f'  ... (å…± {len(instructions)} æ¡æŒ‡ä»¤ï¼Œæ­¤å¤„æ˜¾ç¤ºå‰ {max_instructions} æ¡)')

        if strings:
            prompt_parts.append('')
            prompt_parts.append('é™„è¿‘çš„å­—ç¬¦ä¸²å¸¸é‡:')
            for s in strings[:10]:
                prompt_parts.append(f'  - {s}')

        if called_functions:
            prompt_parts.append('')
            prompt_parts.append('è°ƒç”¨çš„å‡½æ•°:')
            for func in called_functions[:10]:
                prompt_parts.append(f'  - {func}')

        prompt_parts.append('')
        prompt_parts.append('è¯·æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¿”å›åˆ†æç»“æœ:')
        prompt_parts.append('{')
        prompt_parts.append('  "functionality": "è¯¦ç»†çš„åŠŸèƒ½æè¿°ï¼ˆä¸­æ–‡ï¼Œ50-200å­—ï¼Œä»…æè¿°åŠŸèƒ½ï¼Œä¸åŒ…å«æ€§èƒ½åˆ†æï¼‰",')
        prompt_parts.append('  "function_name": "æ¨æ–­çš„å‡½æ•°åï¼ˆè‹±æ–‡ï¼Œéµå¾ªå¸¸è§å‘½åè§„èŒƒï¼‰",')
        prompt_parts.append(
            '  "performance_analysis": "è´Ÿè½½é—®é¢˜è¯†åˆ«ä¸ä¼˜åŒ–å»ºè®®ï¼ˆä¸­æ–‡ï¼Œ100-300å­—ï¼‰ï¼šæ˜¯å¦å­˜åœ¨æ€§èƒ½ç“¶é¢ˆã€ä¸ºä»€ä¹ˆå¯¼è‡´é«˜æŒ‡ä»¤æ•°è´Ÿè½½ã€å¯èƒ½çš„ä¼˜åŒ–å»ºè®®",'
        )
        prompt_parts.append('  "confidence": "é«˜/ä¸­/ä½",')
        prompt_parts.append('  "reasoning": "æ¨ç†è¿‡ç¨‹ï¼ˆä¸­æ–‡ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆè¿™æ ·æ¨æ–­ï¼‰"')
        prompt_parts.append('}')
        prompt_parts.append('')
        prompt_parts.append('æ³¨æ„:')
        prompt_parts.append('1. å¦‚æœç¬¦å·è¡¨ä¸­å·²æœ‰å‡½æ•°åï¼Œä¼˜å…ˆä½¿ç”¨ç¬¦å·åï¼ˆå¦‚æœæ˜¯ C++ åç§°ä¿®é¥°ï¼Œè¯·è¿˜åŸï¼‰')
        prompt_parts.append('2. å‡½æ•°ååº”è¯¥éµå¾ªå¸¸è§çš„å‘½åè§„èŒƒï¼ˆå¦‚é©¼å³°å‘½åã€ä¸‹åˆ’çº¿å‘½åï¼‰')
        prompt_parts.append('3. åŠŸèƒ½æè¿°åº”è¯¥å…·ä½“ï¼Œä¸è¦ä½¿ç”¨æ³›æ³›çš„æè¿°ï¼Œä¸”ä¸è¦åŒ…å«æ€§èƒ½åˆ†æå†…å®¹')
        prompt_parts.append('4. è´Ÿè½½é—®é¢˜è¯†åˆ«ä¸ä¼˜åŒ–å»ºè®®ï¼ˆperformance_analysisï¼‰å¿…é¡»è¯¦ç»†è¯´æ˜ï¼š')
        prompt_parts.append('   - æ˜¯å¦å­˜åœ¨æ˜æ˜¾çš„æ€§èƒ½ç“¶é¢ˆï¼ˆæ˜¯/å¦ï¼Œå¹¶è¯´æ˜åŸå› ï¼‰')
        prompt_parts.append('   - ä¸ºä»€ä¹ˆè¿™ä¸ªå‡½æ•°å¯èƒ½å¯¼è‡´é«˜æŒ‡ä»¤æ•°è´Ÿè½½ï¼ˆå…·ä½“åˆ†æï¼‰')
        prompt_parts.append('   - å¯èƒ½çš„ä¼˜åŒ–å»ºè®®ï¼ˆå¦‚æœæœ‰ï¼‰')
        prompt_parts.append('   ç¤ºä¾‹ï¼š"å­˜åœ¨æ€§èƒ½ç“¶é¢ˆã€‚è¯¥å‡½æ•°åŒ…å«ä¸‰å±‚åµŒå¥—å¾ªç¯ï¼Œæ—¶é—´å¤æ‚åº¦ä¸ºO(nÂ³)ï¼Œ')
        prompt_parts.append(
            '   åœ¨å¤„ç†å¤§é‡æ•°æ®æ—¶ä¼šå¯¼è‡´é«˜æŒ‡ä»¤æ•°è´Ÿè½½ã€‚å»ºè®®ï¼š1) ä¼˜åŒ–ç®—æ³•é™ä½å¤æ‚åº¦ï¼›2) ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤è®¡ç®—"'
        )
        prompt_parts.append('5. ç½®ä¿¡åº¦è¯„ä¼°æ ‡å‡†ï¼š')
        prompt_parts.append(
            "   - 'é«˜'ï¼šèƒ½çœ‹åˆ°å®Œæ•´çš„å‡½æ•°é€»è¾‘ï¼ŒåŒ…æ‹¬å‡½æ•°åºè¨€ã€ä¸»è¦ä¸šåŠ¡é€»è¾‘ã€å‡½æ•°è°ƒç”¨ã€è¿”å›å€¼ç­‰ï¼Œä¸”åŠŸèƒ½æ˜ç¡®"
        )
        prompt_parts.append("   - 'ä¸­'ï¼šèƒ½çœ‹åˆ°éƒ¨åˆ†å‡½æ•°é€»è¾‘ï¼Œèƒ½æ¨æ–­å‡ºå¤§è‡´åŠŸèƒ½ï¼Œä½†å¯èƒ½ç¼ºå°‘ä¸€äº›ç»†èŠ‚")
        prompt_parts.append("   - 'ä½'ï¼šåªèƒ½çœ‹åˆ°å‡½æ•°ç‰‡æ®µï¼ˆå¦‚åªæœ‰å‡½æ•°ç»“å°¾ï¼‰ï¼Œæ— æ³•ç¡®å®šå®Œæ•´åŠŸèƒ½")
        prompt_parts.append(
            "6. å¦‚æœåæ±‡ç¼–ä»£ç ä»å‡½æ•°å¼€å§‹ï¼ˆæœ‰ pacibsp æˆ– stp x29, x30ï¼‰ï¼Œä¸”èƒ½çœ‹åˆ°ä¸»è¦é€»è¾‘ï¼Œç½®ä¿¡åº¦åº”è¯¥è®¾ä¸º'é«˜'æˆ–'ä¸­'"
        )
        prompt_parts.append("7. å¦‚æœæ— æ³•ç¡®å®šï¼Œconfidence è®¾ä¸º'ä½'ï¼Œfunction_name å¯ä»¥ä¸º null")

        return '\n'.join(prompt_parts)

    def _add_open_source_lib_prompt(self, prompt_parts: list[str], open_source_lib: Optional[str] = None):
        """
        æ·»åŠ å¼€æºåº“å’Œ SIMD ç›¸å…³çš„ prompt å†…å®¹ï¼ˆå…±äº«æ–¹æ³•ï¼Œé¿å…é‡å¤ä»£ç ï¼‰

        Args:
            prompt_parts: prompt å†…å®¹åˆ—è¡¨
            open_source_lib: å¼€æºåº“åç§°ï¼ˆå¯é€‰ï¼‰
        """
        if not open_source_lib:
            return

        prompt_parts.append(f'ğŸ” é‡è¦æç¤ºï¼šè¿™æ˜¯ä¸€ä¸ªåŸºäºå¼€æºåº“ {open_source_lib} çš„å®šåˆ¶ç‰ˆæœ¬ï¼ˆä¸‰æ–¹åº“ï¼‰ã€‚')
        prompt_parts.append(
            f'   è¯¥ SO æ–‡ä»¶æ˜¯åŸºäº {open_source_lib} å¼€æºé¡¹ç›®è¿›è¡Œå®šåˆ¶å¼€å‘çš„ï¼Œå‡½æ•°å®ç°å¯èƒ½ä¸æ ‡å‡† {open_source_lib} åº“ç›¸ä¼¼ã€‚'
        )
        prompt_parts.append(f'   è¯·åˆ©ç”¨æ‚¨å¯¹ {open_source_lib} å¼€æºåº“çš„çŸ¥è¯†ï¼Œç»“åˆåç¼–è¯‘ä»£ç çš„ç‰¹å¾ï¼Œ')
        prompt_parts.append(f'   ç›´æ¥æ ¹æ® {open_source_lib} åº“ä¸­å¸¸è§çš„å‡½æ•°åå’ŒåŠŸèƒ½æ¨¡å¼æ¥æ¨æ–­å‡½æ•°åå’ŒåŠŸèƒ½ã€‚')
        prompt_parts.append(f'   å¦‚æœåç¼–è¯‘ä»£ç çš„ç‰¹å¾ä¸ {open_source_lib} åº“ä¸­çš„æŸä¸ªå‡½æ•°åŒ¹é…ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨è¯¥å‡½æ•°åã€‚')
        prompt_parts.append('')
        prompt_parts.append('   ğŸ“Œ SIMD å‘é‡åŒ–æŒ‡ä»¤è¯†åˆ«ï¼ˆé‡è¦ï¼šåŒºåˆ†æ ‡é‡ä¸å‘é‡ï¼‰ï¼š')
        prompt_parts.append('   è¯·ä»”ç»†æ£€æŸ¥åæ±‡ç¼–ä»£ç ä¸­æ˜¯å¦ä½¿ç”¨äº† SIMDï¼ˆå•æŒ‡ä»¤å¤šæ•°æ®ï¼‰å‘é‡åŒ–æŒ‡ä»¤ã€‚')
        prompt_parts.append('')
        prompt_parts.append('   âœ… å‘é‡ NEON æŒ‡ä»¤çš„åˆ¤æ–­æ ‡å‡†ï¼ˆå¿…é¡»åŒæ—¶æ»¡è¶³ï¼‰ï¼š')
        prompt_parts.append('   1. ä½¿ç”¨å‘é‡å¯„å­˜å™¨ v0-v31ï¼ˆä¸æ˜¯æ ‡é‡å¯„å­˜å™¨ s0-s31ï¼‰')
        prompt_parts.append('   2. æœ‰å‘é‡åç¼€æ ‡è¯†ï¼Œå¦‚ï¼š')
        prompt_parts.append('      - .4s, .2s (4ä¸ª/2ä¸ª32ä½æµ®ç‚¹æ•°)')
        prompt_parts.append('      - .8h, .4h (8ä¸ª/4ä¸ª16ä½æ•´æ•°)')
        prompt_parts.append('      - .16b, .8b (16ä¸ª/8ä¸ª8ä½æ•´æ•°)')
        prompt_parts.append('      - .2d (2ä¸ª64ä½æµ®ç‚¹æ•°)')
        prompt_parts.append('   3. å¸¸è§çš„å‘é‡ NEON æŒ‡ä»¤ç¤ºä¾‹ï¼š')
        prompt_parts.append('      - fadd v0.4s, v1.4s, v2.4s  (å‘é‡åŠ æ³•ï¼Œ4ä¸ªæµ®ç‚¹æ•°)')
        prompt_parts.append('      - fmla v0.4s, v1.4s, v2.4s  (å‘é‡èåˆä¹˜åŠ )')
        prompt_parts.append('      - ld1 {v0.4s}, [x0]         (å‘é‡åŠ è½½)')
        prompt_parts.append('      - st1 {v0.4s}, [x0]         (å‘é‡å­˜å‚¨)')
        prompt_parts.append('      - addv s0, v1.4s            (å‘é‡ç¼©å‡)')
        prompt_parts.append('      - dup v0.4s, v1.s[0]        (å‘é‡å¤åˆ¶)')
        prompt_parts.append('')
        prompt_parts.append('   âŒ ä¸æ˜¯å‘é‡ NEON çš„æƒ…å†µï¼š')
        prompt_parts.append('   1. åªä½¿ç”¨æ ‡é‡å¯„å­˜å™¨ s0-s31ï¼ˆ32ä½æµ®ç‚¹ï¼‰æˆ– d0-d31ï¼ˆ64ä½æµ®ç‚¹ï¼‰ï¼Œæ²¡æœ‰å‘é‡åç¼€')
        prompt_parts.append('      - ä¾‹å¦‚ï¼šfmadd s0, s1, s2, s3  (æ ‡é‡èåˆä¹˜åŠ ï¼Œä¸æ˜¯å‘é‡)')
        prompt_parts.append('      - ä¾‹å¦‚ï¼šfadd s0, s1, s2       (æ ‡é‡æµ®ç‚¹åŠ æ³•ï¼Œä¸æ˜¯å‘é‡)')
        prompt_parts.append('      - ä¾‹å¦‚ï¼šldp s0, s1, [x0]     (åŠ è½½ä¸¤ä¸ªæ ‡é‡ï¼Œä¸æ˜¯å‘é‡)')
        prompt_parts.append('   2. è™½ç„¶ fmadd/fnmsub æ˜¯æ€§èƒ½ä¼˜åŒ–æŒ‡ä»¤ï¼Œä½†å¦‚æœåªæ“ä½œæ ‡é‡å¯„å­˜å™¨ï¼Œ')
        prompt_parts.append('      å®ƒä»¬ä¸æ˜¯ SIMD å‘é‡åŒ–æŒ‡ä»¤ï¼Œåªæ˜¯æ ‡é‡æµ®ç‚¹ä¼˜åŒ–æŒ‡ä»¤ã€‚')
        prompt_parts.append('')
        prompt_parts.append('   ğŸ” åˆ¤æ–­æ­¥éª¤ï¼š')
        prompt_parts.append('   1. é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä½¿ç”¨ v0-v31 å‘é‡å¯„å­˜å™¨')
        prompt_parts.append('   2. ç„¶åæ£€æŸ¥æ˜¯å¦æœ‰å‘é‡åç¼€ï¼ˆ.4s, .2s, .8h ç­‰ï¼‰')
        prompt_parts.append('   3. åªæœ‰åŒæ—¶æ»¡è¶³ä»¥ä¸Šä¸¤ç‚¹ï¼Œæ‰æ˜¯å‘é‡ NEON æŒ‡ä»¤')
        prompt_parts.append('   4. å¦‚æœåªä½¿ç”¨ s/d å¯„å­˜å™¨ä¸”æ— å‘é‡åç¼€ï¼Œåˆ™æ˜¯æ ‡é‡æŒ‡ä»¤ï¼Œä¸æ˜¯å‘é‡ NEON')
        prompt_parts.append('')
        prompt_parts.append('   å¦‚æœè¯†åˆ«åˆ°å‘é‡ NEON æŒ‡ä»¤çš„ä½¿ç”¨ï¼Œè¯·åœ¨åŠŸèƒ½æè¿°ä¸­æ˜ç¡®è¯´æ˜è¯¥å‡½æ•°ä½¿ç”¨äº†å‘é‡åŒ–ä¼˜åŒ–ï¼Œ')
        prompt_parts.append('   å¹¶æè¿°å‘é‡åŒ–çš„å…·ä½“ç”¨é€”ï¼ˆå¦‚æ‰¹é‡æ•°æ®å¤„ç†ã€å¹¶è¡Œè®¡ç®—ã€çŸ©é˜µè¿ç®—ç­‰ï¼‰ã€‚')
        prompt_parts.append('')
        prompt_parts.append('   ğŸ“Œ å¼€æºåº“åŠŸèƒ½ç›¸å…³æ€§ï¼š')
        prompt_parts.append(f'   åœ¨åŠŸèƒ½æè¿°ä¸­ï¼Œè¯·ç»“åˆ {open_source_lib} å¼€æºåº“çš„å…¸å‹åŠŸèƒ½å’Œç‰¹æ€§ï¼Œ')
        prompt_parts.append(
            f'   è¯´æ˜è¯¥å‡½æ•°åœ¨ {open_source_lib} åº“ä¸­çš„ä½œç”¨å’Œå®šä½ï¼Œä»¥åŠä¸å…¶ä»– {open_source_lib} å‡½æ•°çš„å…³è”æ€§ã€‚'
        )
        prompt_parts.append(f'   ä¾‹å¦‚ï¼šè¯¥å‡½æ•°æ˜¯ {open_source_lib} ä¸­å“ªä¸ªæ¨¡å—/ç»„ä»¶çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œ')
        prompt_parts.append(f'   åœ¨ {open_source_lib} çš„å…¸å‹ä½¿ç”¨åœºæ™¯ä¸­æ‰®æ¼”ä»€ä¹ˆè§’è‰²ï¼Œ')
        prompt_parts.append(f'   ä¸ {open_source_lib} åº“ä¸­å“ªäº›å¸¸è§å‡½æ•°æˆ–åŠŸèƒ½ç›¸å…³ã€‚')
        prompt_parts.append('')

    def _parse_llm_response(self, response_text: str) -> dict[str, Any]:
        """è§£æ LLM è¿”å›çš„ç»“æœ"""
        # å°è¯•æå– JSON
        result = {
            'functionality': 'æœªçŸ¥',
            'function_name': None,
            'performance_analysis': '',
            'confidence': 'ä½',
            'reasoning': 'è§£æå¤±è´¥',
        }

        # æ£€æŸ¥å“åº”æ˜¯å¦å¯èƒ½è¢«æˆªæ–­
        is_truncated = False
        if response_text.endswith('\\') or response_text.endswith('...') or not response_text.rstrip().endswith('}'):
            is_truncated = True

        # å°è¯•æ‰¾åˆ° JSON å—
        try:
            # æŸ¥æ‰¾ JSON å—
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1

            if start_idx >= 0 and end_idx > start_idx:
                json_str = response_text[start_idx:end_idx]

                # å¦‚æœå“åº”å¯èƒ½è¢«æˆªæ–­ï¼Œå°è¯•ä¿®å¤ JSON
                if (
                    is_truncated
                    and not json_str.rstrip().endswith('}')
                    and '"functionality"' in json_str
                    and '"reasoning"' in json_str
                ):
                    # æŸ¥æ‰¾ reasoning å­—æ®µçš„å¼€å§‹
                    reasoning_start = json_str.rfind('"reasoning"')
                    if reasoning_start > 0:
                        # å°è¯•è¡¥å…¨ JSONï¼ˆæ·»åŠ ç¼ºå¤±çš„å¼•å·å’Œæ‹¬å·ï¼‰
                        if not json_str.rstrip().endswith('"'):
                            # æ‰¾åˆ°æœ€åä¸€ä¸ªæœªé—­åˆçš„å­—ç¬¦ä¸²
                            last_quote = json_str.rfind('"')
                            if last_quote > reasoning_start:
                                # åœ¨æœ€åä¸€ä¸ªå¼•å·åè¡¥å…¨
                                json_str = json_str[: last_quote + 1] + '}'
                            else:
                                # å¦‚æœæ‰¾ä¸åˆ°å¼•å·ï¼Œå°è¯•åœ¨æœ«å°¾è¡¥å…¨
                                json_str = json_str.rstrip().rstrip(',') + '}'
                        else:
                            json_str = json_str.rstrip().rstrip(',') + '}'

                try:
                    parsed = json.loads(json_str)

                    result['functionality'] = parsed.get('functionality', 'æœªçŸ¥')
                    result['function_name'] = parsed.get('function_name')
                    result['performance_analysis'] = parsed.get('performance_analysis', '')
                    result['confidence'] = parsed.get('confidence', 'ä½')
                    result['reasoning'] = parsed.get('reasoning', '')

                    # è°ƒè¯•ï¼šæ£€æŸ¥ performance_analysis æ˜¯å¦å­˜åœ¨
                    if not result['performance_analysis']:
                        logger.warning(
                            'âš ï¸  LLM å“åº”ä¸­æœªåŒ…å« performance_analysis å­—æ®µæˆ–ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ prompt æ˜¯å¦åŒ…å«è¯¥å­—æ®µè¦æ±‚'
                        )
                        # å°è¯•ä» reasoning æˆ–å…¶ä»–å­—æ®µä¸­æå–æ€§èƒ½ç›¸å…³ä¿¡æ¯ï¼ˆfallbackï¼‰
                        if result.get('reasoning') and (
                            'æ€§èƒ½' in result['reasoning']
                            or 'ç“¶é¢ˆ' in result['reasoning']
                            or 'è´Ÿè½½' in result['reasoning']
                        ):
                            logger.debug(
                                'Detected performance-related information in reasoning field, but performance_analysis is empty'
                            )

                    # å¦‚æœæ£€æµ‹åˆ°æˆªæ–­ï¼Œåœ¨ç»“æœä¸­æ ‡è®°
                    if is_truncated:
                        if result['functionality'].endswith('\\') or len(result['functionality']) < 100:
                            result['functionality'] += ' [å“åº”å¯èƒ½è¢«æˆªæ–­]'
                        if result['reasoning'].endswith('\\') or len(result['reasoning']) < 200:
                            result['reasoning'] += ' [å“åº”å¯èƒ½è¢«æˆªæ–­]'
                except json.JSONDecodeError as je:
                    # JSON è§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–
                    logger.warning('Warning: JSON parsing failed, attempting text extraction: %s', str(je)[:100])
                    result['reasoning'] = f'JSONè§£æå¤±è´¥: {str(je)[:100]}\nåŸå§‹å“åº”: {response_text[:500]}'

                    # å°è¯•ä»æ–‡æœ¬ä¸­æå–å­—æ®µ
                    name_match = re.search(
                        r'["\']?function_name["\']?\s*:\s*["\']([^"\']+)["\']',
                        response_text,
                    )
                    if name_match:
                        result['function_name'] = name_match.group(1).strip()

                    func_match = re.search(
                        r'["\']?functionality["\']?\s*:\s*["\']([^"\']+)["\']',
                        response_text,
                    )
                    if func_match:
                        result['functionality'] = func_match.group(1).strip()
            else:
                # å¦‚æœæ²¡æœ‰ JSONï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–
                result['reasoning'] = response_text
                # å°è¯•æå–å‡½æ•°åï¼ˆæŸ¥æ‰¾ç±»ä¼¼ "function_name": "xxx" çš„æ¨¡å¼ï¼‰
                name_match = re.search(
                    r'["\']?function_name["\']?\s*:\s*["\']?([^"\',\n}]+)',
                    response_text,
                )
                if name_match:
                    result['function_name'] = name_match.group(1).strip()

                func_match = re.search(
                    r'["\']?functionality["\']?\s*:\s*["\']?([^"\',\n}]+)',
                    response_text,
                )
                if func_match:
                    result['functionality'] = func_match.group(1).strip()
        except Exception as e:
            result['reasoning'] = f'è§£æå¤±è´¥: {str(e)}\nåŸå§‹å“åº”: {response_text[:500]}'

        return result

    def batch_analyze(self, analysis_tasks: list[dict[str, Any]], delay: float = 0.5) -> list[dict[str, Any]]:
        """
        æ‰¹é‡åˆ†æå¤šä¸ªå‡½æ•°ï¼ˆå¸¦å»¶è¿Ÿä»¥é¿å… API é™æµï¼‰

        Args:
            analysis_tasks: åˆ†æä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«:
                {
                    'instructions': [...],
                    'strings': [...],
                    'symbol_name': ...,
                    'called_functions': [...],
                    'offset': ...
                }
            delay: æ¯æ¬¡ API è°ƒç”¨ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰

        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        results = []
        total = len(analysis_tasks)

        for i, task in enumerate(analysis_tasks):
            if i > 0:
                time.sleep(delay)  # é¿å… API é™æµ

            if (i + 1) % 10 == 0:
                logger.info(f'  LLM analysis progress: {i + 1}/{total}')

            result = self.analyze_with_llm(
                instructions=task.get('instructions', []),
                strings=task.get('strings', []),
                symbol_name=task.get('symbol_name'),
                called_functions=task.get('called_functions', []),
                offset=task.get('offset'),
            )
            results.append(result)

        return results

    def finalize(self):
        """å®Œæˆåˆ†æåè°ƒç”¨ï¼Œä¿å­˜æ‰€æœ‰ç¼“å­˜å’Œç»Ÿè®¡ä¿¡æ¯"""
        if self.enable_cache:
            self._save_cache()
        self._save_token_stats()

    def _save_single_prompt(self, prompt: str, offset: Optional[int], symbol_name: Optional[str]):
        """ä¿å­˜å•ä¸ªå‡½æ•°çš„ prompt åˆ°æ–‡ä»¶"""
        if not self.save_prompts:
            return

        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]  # åŒ…å«æ¯«ç§’

            # ç”Ÿæˆæ–‡ä»¶å
            offset_str = f'0x{offset:x}' if offset else 'unknown'
            symbol_str = symbol_name.replace('/', '_').replace('\\', '_') if symbol_name else 'unknown'
            # é™åˆ¶æ–‡ä»¶åé•¿åº¦
            if len(symbol_str) > 50:
                symbol_str = symbol_str[:50]

            prompt_file = self.prompt_output_dir / f'prompt_{offset_str}_{symbol_str}_{timestamp}.txt'

            with open(prompt_file, 'w', encoding='utf-8') as f:
                f.write('=' * 80 + '\n')
                f.write('ç”Ÿæˆçš„ LLM Prompt (å•ä¸ªå‡½æ•°)\n')
                f.write('=' * 80 + '\n')
                f.write(f'ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}\n')
                f.write(f'å‡½æ•°åç§»é‡: {offset_str}\n')
                if symbol_name:
                    f.write(f'ç¬¦å·å: {symbol_name}\n')
                f.write(f'æ€»é•¿åº¦: {len(prompt):,} å­—ç¬¦\n')
                f.write(f'ä¼°è®¡ Token æ•°: â‰ˆ{len(prompt) // 4:,}\n')
                f.write('=' * 80 + '\n\n')
                f.write(prompt)
                f.write('\n\n' + '=' * 80 + '\n')

            logger.debug(f'Prompt saved: {prompt_file.name}')
        except Exception as e:
            logger.warning(f'Failed to save prompt: {e}')
