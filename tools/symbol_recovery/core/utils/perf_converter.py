#!/usr/bin/env python3
"""
ä»perf.data æˆ– excel æ–‡ä»¶è¯»å–ç¼ºå¤±ç¬¦å·çš„å‡½æ•°ï¼Œè¿›è¡Œåæ±‡ç¼–å’ŒLLMåˆ†æ
"""

import os
import shutil
import sqlite3
from collections import defaultdict
from pathlib import Path
from typing import Optional

import pandas as pd
from elftools.elf.elffile import ELFFile

try:
    from openpyxl import Workbook
    from openpyxl.styles import Alignment
    from openpyxl.utils import get_column_letter
except ImportError:  # pragma: no cover - optional dependency
    Workbook = None
    Alignment = None
    get_column_letter = None

try:
    from core.llm.batch_analyzer import BatchLLMFunctionAnalyzer
except ImportError:  # pragma: no cover - optional dependency
    BatchLLMFunctionAnalyzer = None
from core.llm.initializer import init_llm_analyzer
from core.utils import StringExtractor, get_logger
from core.utils import common as util
from core.utils.config import (
    CALL_COUNT_ANALYSIS_PATTERN,
    DEFAULT_BATCH_SIZE,
    DEFAULT_LLM_MODEL,
    DEFAULT_TOP_N,
    config,
)

logger = get_logger(__name__)

# HAP åœ°å€è§£æï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–ï¼‰
try:
    from core.utils.hap_address_resolver import (
        is_hap_address,
        resolve_hap_address_from_perfdb,
        resolve_hap_addresses_batch,
    )

    HAP_RESOLVER_AVAILABLE = True
except ImportError:
    HAP_RESOLVER_AVAILABLE = False
    is_hap_address = None
    resolve_hap_address_from_perfdb = None
    resolve_hap_addresses_batch = None
    logger.warning('HAP address resolver module unavailable')

try:
    import r2pipe

    from core.analyzers.r2_analyzer import R2FunctionAnalyzer

    R2_AVAILABLE = True
except ImportError:
    R2_AVAILABLE = False
    R2FunctionAnalyzer = None
    r2pipe = None
    logger.warning('r2_function_analyzer module unavailable, will use capstone for disassembly and LLM analysis')


def _check_r2_actually_available():
    """
    æ£€æµ‹ radare2 æ˜¯å¦çœŸçš„å¯ç”¨ï¼ˆä¸ä»…ä»…æ˜¯æ¨¡å—æ˜¯å¦å¯å¯¼å…¥ï¼‰
    é€šè¿‡æ£€æŸ¥ r2 å‘½ä»¤æ˜¯å¦åœ¨ PATH ä¸­æ¥åˆ¤æ–­

    Returns:
        bool: å¦‚æœ radare2 å¯ç”¨è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
    """
    if not R2_AVAILABLE:
        return False

    try:
        # æ£€æŸ¥ r2 å‘½ä»¤æ˜¯å¦åœ¨ PATH ä¸­
        r2_path = shutil.which('r2')
        if r2_path:
            logger.debug(f'Found radare2 command: {r2_path}')
            return True

        # å¦‚æœæ‰¾ä¸åˆ° r2 å‘½ä»¤ï¼Œè¯´æ˜ radare2 ä¸å¯ç”¨
        logger.debug('radare2 command not found (r2 not in PATH)')
        return False
    except Exception as e:
        logger.debug(f'Error checking radare2 availability: {e}')
        return False


class MissingSymbolFunctionAnalyzer:
    """åˆ†æç¼ºå¤±ç¬¦å·çš„å‡½æ•°"""

    def __init__(
        self,
        excel_file=None,
        so_dir=None,
        perf_db_file=None,
        use_llm=True,
        llm_model=None,
        batch_size=None,
        context=None,
        use_capstone_only=False,
        save_prompts=False,
        output_dir=None,
        skip_decompilation=False,
        open_source_lib=None,
    ):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            excel_file: ç¼ºå¤±ç¬¦å·åˆ†æ Excel æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾› perf_db_file åˆ™ä¸éœ€è¦ï¼‰
            so_dir: SO æ–‡ä»¶ç›®å½•
            perf_db_file: perf.db æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ç›´æ¥ä»æ•°æ®åº“è¯»å–ï¼Œä¸éœ€è¦ Excelï¼‰
            use_llm: æ˜¯å¦ä½¿ç”¨ LLM åˆ†æ
            llm_model: LLM æ¨¡å‹åç§°
            use_batch_llm: æ˜¯å¦ä½¿ç”¨æ‰¹é‡ LLM åˆ†æï¼ˆä¸€ä¸ª prompt åŒ…å«å¤šä¸ªå‡½æ•°ï¼Œé»˜è®¤ Trueï¼‰
            batch_size: æ‰¹é‡åˆ†ææ—¶æ¯ä¸ª prompt åŒ…å«çš„å‡½æ•°æ•°é‡ï¼ˆé»˜è®¤ 3ï¼‰
            context: è‡ªå®šä¹‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™æ ¹æ® SO æ–‡ä»¶åè‡ªåŠ¨æ¨æ–­ï¼‰
            use_capstone_only: åªä½¿ç”¨ Capstone åæ±‡ç¼–ï¼ˆä¸ä½¿ç”¨ Radare2ï¼Œå³ä½¿å·²å®‰è£…ï¼‰
            save_prompts: æ˜¯å¦ä¿å­˜ç”Ÿæˆçš„ prompt åˆ°æ–‡ä»¶
            output_dir: è¾“å‡ºç›®å½•ï¼Œç”¨äºä¿å­˜ prompt æ–‡ä»¶
            skip_decompilation: æ˜¯å¦è·³è¿‡åç¼–è¯‘ï¼ˆé»˜è®¤ Falseï¼Œå¯ç”¨åç¼–è¯‘å¯æé«˜ LLM åˆ†æè´¨é‡ä½†è¾ƒæ…¢ï¼‰
        """
        self.excel_file = Path(excel_file) if excel_file else None
        self.perf_db_file = Path(perf_db_file) if perf_db_file else None
        self.so_dir = Path(so_dir) if so_dir else None
        self.use_llm = use_llm
        self.llm_model = llm_model if llm_model is not None else DEFAULT_LLM_MODEL
        self.batch_size = batch_size if batch_size is not None else DEFAULT_BATCH_SIZE
        self.context = context  # è‡ªå®šä¹‰ä¸Šä¸‹æ–‡
        self.use_capstone_only = use_capstone_only  # å¼ºåˆ¶ä½¿ç”¨ Capstone
        self.skip_decompilation = skip_decompilation  # æ˜¯å¦è·³è¿‡åç¼–è¯‘
        self._r2_actually_available = None  # ç¼“å­˜ radare2 å®é™…å¯ç”¨æ€§ï¼ˆå»¶è¿Ÿæ£€æµ‹ï¼‰

        # éªŒè¯è¾“å…¥ï¼šå¿…é¡»æä¾› excel_file æˆ– perf_db_file ä¹‹ä¸€
        if not self.excel_file and not self.perf_db_file:
            raise ValueError('å¿…é¡»æä¾› excel_file æˆ– perf_db_file ä¹‹ä¸€')

        if self.excel_file and not self.excel_file.exists():
            raise FileNotFoundError(f'Excel æ–‡ä»¶ä¸å­˜åœ¨: {excel_file}')

        if self.perf_db_file and not self.perf_db_file.exists():
            raise FileNotFoundError(f'perf.db æ–‡ä»¶ä¸å­˜åœ¨: {perf_db_file}')

        if self.so_dir:
            # å¦‚æœ so_dir æ˜¯æ–‡ä»¶ï¼ˆé€šè¿‡ --so-file æŒ‡å®šï¼‰ï¼Œå…è®¸ï¼›å¦‚æœæ˜¯ç›®å½•ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨
            if not self.so_dir.exists():
                raise FileNotFoundError(f'SO æ–‡ä»¶æˆ–ç›®å½•ä¸å­˜åœ¨: {so_dir}')
            if self.so_dir.is_file() and not self.so_dir.suffix == '.so':
                raise ValueError(f'æŒ‡å®šçš„æ–‡ä»¶ä¸æ˜¯ SO æ–‡ä»¶: {so_dir}')

        # åˆå§‹åŒ–åæ±‡ç¼–å™¨
        self.md = util.create_disassembler()

        # åˆå§‹åŒ–å­—ç¬¦ä¸²æå–å™¨ï¼ˆç¨åè®¾ç½® disassemble_funcï¼‰
        self.string_extractor = None

        # ç¼“å­˜ radare2 åˆ†æå™¨å®ä¾‹ï¼ˆæŒ‰ SO æ–‡ä»¶è·¯å¾„ç¼“å­˜ï¼Œé¿å…é‡å¤åˆå§‹åŒ–ï¼‰
        self._r2_analyzers = {}  # {so_file_path: R2FunctionAnalyzer}

        # åˆå§‹åŒ– LLM åˆ†æå™¨ï¼ˆä½¿ç”¨å…¬å…±å·¥å…·å‡½æ•°ï¼Œé¿å…é‡å¤ä»£ç ï¼‰
        self.llm_analyzer, self.use_llm, self.use_batch_llm = init_llm_analyzer(
            use_llm=self.use_llm,
            llm_model=self.llm_model,
            batch_size=self.batch_size,
            save_prompts=save_prompts,
            output_dir=output_dir,
            open_source_lib=open_source_lib,
        )

    def find_so_file(self, file_path):
        """
        æ ¹æ®æ–‡ä»¶è·¯å¾„æ‰¾åˆ°å¯¹åº”çš„ SO æ–‡ä»¶

        Args:
            file_path: æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ /proc/.../libxwebcore.soï¼‰

        Returns:
            SO æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæ‰¾ä¸åˆ°è¿”å› None
        """
        # å¦‚æœ so_dir æ˜¯ä¸€ä¸ªæ–‡ä»¶ï¼ˆé€šè¿‡ --so-file æŒ‡å®šï¼‰ï¼Œç›´æ¥è¿”å›å®ƒï¼ˆå¦‚æœæ–‡ä»¶ååŒ¹é…ï¼‰
        if self.so_dir and self.so_dir.is_file():
            # æå–ç›®æ ‡æ–‡ä»¶åï¼ˆä» file_pathï¼‰
            target_so_name = file_path.split('/')[-1] if '/' in file_path else file_path
            # å¦‚æœæŒ‡å®šçš„ SO æ–‡ä»¶ååŒ¹é…ï¼Œç›´æ¥è¿”å›
            if self.so_dir.name == target_so_name:
                return self.so_dir.resolve()
            # å¦‚æœä¸åŒ¹é…ï¼Œè¿”å› Noneï¼ˆåªåˆ†ææŒ‡å®šçš„ SO æ–‡ä»¶ï¼‰
            # æ³¨æ„ï¼šè¿™é‡Œä¸åŒ¹é…å¯èƒ½æ˜¯å› ä¸º file_path æ¥è‡ªè°ƒç”¨æ ˆï¼Œè€Œä¸æ˜¯å‡½æ•°æ‰€åœ¨çš„æ–‡ä»¶
            # çœŸæ­£çš„ SO æ–‡ä»¶åå¯èƒ½åœ¨ address ä¸­ï¼Œä½†è¿™é‡Œæˆ‘ä»¬æ— æ³•è®¿é—® address
            # æ‰€ä»¥è¿™ä¸ªæ£€æŸ¥ä¼šåœ¨ analyze_function è°ƒç”¨æ—¶å†æ¬¡è¿›è¡Œ
            return None

        # æå–æ–‡ä»¶åï¼ˆå¦‚ libxwebcore.soï¼‰
        so_name = None
        so_name = file_path.split('/')[-1] if '/' in file_path else file_path

        # åœ¨æŒ‡å®šç›®å½•ä¸­æŸ¥æ‰¾
        so_file = self.so_dir / so_name
        if so_file.exists():
            # è¿”å›ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿è·¯å¾„ä¸€è‡´æ€§ï¼ˆç”¨äºç¼“å­˜é”®ï¼‰
            return so_file.resolve()

        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•é€’å½’æŸ¥æ‰¾æ‰€æœ‰å­ç›®å½•ä¸­çš„ SO æ–‡ä»¶
        for so_file in self.so_dir.rglob('*.so'):
            if so_file.name == so_name:
                # è¿”å›ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿è·¯å¾„ä¸€è‡´æ€§ï¼ˆç”¨äºç¼“å­˜é”®ï¼‰
                return so_file.resolve()

        return None

    def extract_offset_from_address(self, address):
        """
        ä»åœ°å€å­—ç¬¦ä¸²ä¸­æå–åç§»é‡ï¼ˆè™šæ‹Ÿåœ°å€ï¼‰
        ä½¿ç”¨ç»Ÿä¸€çš„ parse_offset å‡½æ•°
        """
        return util.parse_offset(address)

    def vaddr_to_file_offset(self, elf_file, vaddr):
        """å°†è™šæ‹Ÿåœ°å€è½¬æ¢ä¸ºæ–‡ä»¶åç§»é‡"""
        for segment in elf_file.iter_segments():
            if segment['p_type'] == 'PT_LOAD':
                vaddr_start = segment['p_vaddr']
                vaddr_end = vaddr_start + segment['p_memsz']
                if vaddr_start <= vaddr < vaddr_end:
                    return segment['p_offset'] + (vaddr - vaddr_start)
        return None

    def find_function_start(self, elf_file, vaddr):
        """æŸ¥æ‰¾å‡½æ•°çš„èµ·å§‹ä½ç½®ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„ util.find_function_startï¼‰"""
        return util.find_function_start(elf_file, vaddr, self.md)

    def disassemble_function(self, elf_file, vaddr, size=2000):
        """åæ±‡ç¼–æŒ‡å®šè™šæ‹Ÿåœ°å€çš„å‡½æ•°ä»£ç 

        Args:
            elf_file: ELF æ–‡ä»¶å¯¹è±¡
            vaddr: å‡½æ•°è™šæ‹Ÿåœ°å€
            size: æœ€å¤§åæ±‡ç¼–å¤§å°ï¼ˆé»˜è®¤ 2000 å­—èŠ‚ï¼Œå¢åŠ ä»¥æ”¯æŒå¤§å‹å‡½æ•°ï¼‰
        """
        try:
            # è·å– .text æ®µ
            text_section = None
            for section in elf_file.iter_sections():
                if section.name == '.text':
                    text_section = section
                    break

            if not text_section:
                return None

            text_vaddr = text_section['sh_addr']
            text_size = text_section['sh_size']

            # ç¡®ä¿åœ°å€åœ¨ .text æ®µå†…
            if vaddr < text_vaddr or vaddr >= text_vaddr + text_size:
                logger.warning(' Address 0x%s is not in .text section', f'{vaddr:x}')
                return None

            # æŸ¥æ‰¾å‡½æ•°èµ·å§‹ä½ç½®
            func_start = self.find_function_start(elf_file, vaddr)

            # å°è¯•ä»ç¬¦å·è¡¨è·å–å‡½æ•°å¤§å°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            func_size = None
            try:
                # æŸ¥æ‰¾ .dynsym æˆ– .symtab ç¬¦å·è¡¨
                for section_name in ['.dynsym', '.symtab']:
                    symbol_table = elf_file.get_section_by_name(section_name)
                    if symbol_table:
                        for symbol in symbol_table.iter_symbols():
                            if symbol['st_info']['type'] == 'STT_FUNC':
                                sym_addr = symbol['st_value']
                                sym_size = symbol['st_size']
                                # æ£€æŸ¥åœ°å€æ˜¯å¦åœ¨è¿™ä¸ªç¬¦å·èŒƒå›´å†…
                                if sym_size > 0 and sym_addr <= vaddr < sym_addr + sym_size:
                                    func_size = sym_size
                                    logger.info(
                                        f'Got function size from symbol table: {func_size} bytes (symbol: {symbol.name})'
                                    )
                                    break
                    if func_size:
                        break
            except Exception:
                # ç¬¦å·è¡¨æŸ¥æ‰¾å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å¤§å°
                pass

            # è®¡ç®—ç›¸å¯¹åç§»é‡
            relative_start = func_start - text_vaddr
            if func_size:
                # ä½¿ç”¨ç¬¦å·è¡¨ä¸­çš„å‡½æ•°å¤§å°ï¼Œä½†ä¸è¶…è¿‡ size é™åˆ¶
                relative_end = min(relative_start + func_size, relative_start + size, text_size)
            else:
                relative_end = min(relative_start + size, text_size)

            # è¯»å–ä»£ç 
            code = text_section.data()[relative_start:relative_end]

            if not code:
                return None

            # åæ±‡ç¼–
            instructions = []
            ret_count = 0  # è®°å½•é‡åˆ°çš„ ret æŒ‡ä»¤æ•°é‡
            consecutive_ret = 0  # è¿ç»­ ret æŒ‡ä»¤è®¡æ•°

            for inst in self.md.disasm(code, func_start):
                instructions.append(inst)

                # æ”¹è¿›çš„åœæ­¢æ¡ä»¶ï¼š
                # 1. å¦‚æœé‡åˆ° ret æŒ‡ä»¤ï¼Œè®°å½•ä½†ä¸ç«‹å³åœæ­¢
                # 2. å¦‚æœè¿ç»­é‡åˆ°å¤šä¸ª retï¼ˆå¯èƒ½æ˜¯å‡½æ•°ç»“æŸæ ‡è®°ï¼‰ï¼Œä¸”å·²åæ±‡ç¼–è¶³å¤ŸæŒ‡ä»¤ï¼Œåˆ™åœæ­¢
                # 3. å¦‚æœé‡åˆ° ret ä¸”å·²ç»åæ±‡ç¼–äº†å¤§éƒ¨åˆ†å‡½æ•°ï¼ˆè¶…è¿‡ 80%ï¼‰ï¼Œåˆ™åœæ­¢
                if inst.mnemonic == 'ret':
                    ret_count += 1
                    consecutive_ret += 1
                    # å¦‚æœè¿ç»­é‡åˆ° 2 ä¸ª retï¼Œä¸”å·²åæ±‡ç¼–è¶…è¿‡ 300 æ¡æŒ‡ä»¤ï¼ˆçº¦ 1200 å­—èŠ‚ï¼‰ï¼Œå¯èƒ½æ˜¯å‡½æ•°ç»“æŸ
                    # æ³¨æ„ï¼šARM64 æŒ‡ä»¤é€šå¸¸æ˜¯ 4 å­—èŠ‚ï¼Œ300 æ¡æŒ‡ä»¤ â‰ˆ 1200 å­—èŠ‚
                    # è¿™ä¸ªé˜ˆå€¼é€‚ç”¨äºå¤§å¤šæ•°å‡½æ•°ï¼Œä½†å¤æ‚å‡½æ•°å¯èƒ½è¶…è¿‡è¿™ä¸ªå€¼
                    if consecutive_ret >= 2 and len(instructions) > 300:
                        break
                    # å¦‚æœé‡åˆ° ret ä¸”å·²åæ±‡ç¼–è¶…è¿‡ size çš„ 80%ï¼Œå¯èƒ½æ˜¯å‡½æ•°ç»“æŸ
                    if len(instructions) * 4 > (relative_end - relative_start) * 0.8:
                        break
                else:
                    consecutive_ret = 0  # é‡ç½®è¿ç»­ ret è®¡æ•°

                # å¦‚æœå·²ç»åæ±‡ç¼–äº†è¶³å¤Ÿå¤šçš„æŒ‡ä»¤ï¼ˆè¶…è¿‡ size é™åˆ¶ï¼‰ï¼Œåœæ­¢
                if len(instructions) * 4 > (relative_end - relative_start):
                    break

            return instructions
        except Exception:
            logger.exception('åæ±‡ç¼–å¤±è´¥ (vaddr=0x%x)', vaddr)
            return None

    def _init_string_extractor(self):
        """å»¶è¿Ÿåˆå§‹åŒ–å­—ç¬¦ä¸²æå–å™¨ï¼ˆéœ€è¦å…ˆå®šä¹‰ disassemble_functionï¼‰"""
        if self.string_extractor is None:
            self.string_extractor = StringExtractor(disassemble_func=self.disassemble_function, md=self.md)

    def _build_context(self, so_file, file_path=None):
        """
        æ„å»º LLM åˆ†æçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆæ ¹æ® SO æ–‡ä»¶åå’Œåº”ç”¨è·¯å¾„è‡ªåŠ¨æ¨æ–­ï¼‰

        Args:
            so_file: SO æ–‡ä»¶è·¯å¾„
            file_path: æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºæ¨æ–­åº”ç”¨ç±»å‹ï¼‰

        Returns:
            ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        so_name = Path(so_file).name.lower()
        so_file_name = Path(so_file).name

        # ä»æ–‡ä»¶è·¯å¾„æ¨æ–­åº”ç”¨ç±»å‹
        app_name = None
        if file_path:
            file_path_lower = file_path.lower()
            if 'taobao' in file_path_lower or 'com.taobao' in file_path_lower:
                app_name = 'æ·˜å®ï¼ˆTaobaoï¼‰'
            elif 'wechat' in file_path_lower or 'com.tencent.wechat' in file_path_lower:
                app_name = 'å¾®ä¿¡ï¼ˆWeChatï¼‰'
            elif 'alipay' in file_path_lower or 'com.alipay' in file_path_lower:
                app_name = 'æ”¯ä»˜å®ï¼ˆAlipayï¼‰'
            elif 'qq' in file_path_lower and 'com.tencent' in file_path_lower:
                app_name = 'QQ'
            elif 'douyin' in file_path_lower or 'com.ss.android' in file_path_lower:
                app_name = 'æŠ–éŸ³ï¼ˆDouyinï¼‰'

        # æ ¹æ® SO æ–‡ä»¶åæ¨æ–­åº“çš„ç±»å‹å’Œç”¨é€”
        if 'xwebcore' in so_name or 'xweb' in so_name:
            context = (
                f'è¿™æ˜¯ä¸€ä¸ªåŸºäº Chromium Embedded Framework (CEF) çš„ Web æ ¸å¿ƒåº“ï¼ˆ{so_file_name}ï¼‰ï¼Œ'
                f'è¿è¡Œåœ¨ HarmonyOS å¹³å°ä¸Šã€‚è¯¥åº“è´Ÿè´£ç½‘é¡µæ¸²æŸ“ã€ç½‘ç»œè¯·æ±‚ã€DOM æ“ä½œç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚'
            )
        elif 'wechat' in so_name or 'wx' in so_name:
            context = (
                f'è¿™æ˜¯ä¸€ä¸ªæ¥è‡ªå¾®ä¿¡ï¼ˆWeChatï¼‰åº”ç”¨çš„ SO åº“ï¼ˆ{so_file_name}ï¼‰ï¼Œ'
                f'è¿è¡Œåœ¨ HarmonyOS å¹³å°ä¸Šã€‚è¯¥åº“è´Ÿè´£å³æ—¶é€šè®¯ã€ç¤¾äº¤ç½‘ç»œã€å¤šåª’ä½“å¤„ç†ç­‰åŠŸèƒ½ã€‚'
            )
        elif 'taobao' in so_name or 'tb' in so_name:
            context = (
                f'è¿™æ˜¯ä¸€ä¸ªæ¥è‡ªæ·˜å®ï¼ˆTaobaoï¼‰åº”ç”¨çš„ SO åº“ï¼ˆ{so_file_name}ï¼‰ï¼Œ'
                f'è¿è¡Œåœ¨ HarmonyOS å¹³å°ä¸Šã€‚è¯¥åº“è´Ÿè´£ç”µå•†è´­ç‰©ã€å•†å“å±•ç¤ºã€æ”¯ä»˜å¤„ç†ç­‰åŠŸèƒ½ã€‚'
            )
        elif 'chromium' in so_name or 'blink' in so_name or 'v8' in so_name:
            context = (
                f'è¿™æ˜¯ä¸€ä¸ªåŸºäº Chromium/Blink å¼•æ“çš„ç»„ä»¶åº“ï¼ˆ{so_file_name}ï¼‰ï¼Œ'
                f'é€šå¸¸ç”¨äº Web æ¸²æŸ“ã€JavaScript æ‰§è¡Œç­‰ Web ç›¸å…³åŠŸèƒ½ã€‚'
            )
        elif 'flutter' in so_name:
            context = (
                f'è¿™æ˜¯ä¸€ä¸ª Flutter æ¡†æ¶ç›¸å…³çš„ SO åº“ï¼ˆ{so_file_name}ï¼‰ï¼Œ'
                f'Flutter æ˜¯ Google å¼€å‘çš„è·¨å¹³å° UI æ¡†æ¶ï¼Œç”¨äºæ„å»ºç§»åŠ¨åº”ç”¨ç•Œé¢ã€‚'
            )
        # é€šç”¨æ ¼å¼ï¼Œæ ¹æ®åº”ç”¨åç§°è°ƒæ•´
        elif app_name:
            context = f'è¿™æ˜¯ä¸€ä¸ªæ¥è‡ª {app_name} åº”ç”¨çš„ SO åº“ï¼ˆ{so_file_name}ï¼‰ï¼Œè¿è¡Œåœ¨ HarmonyOS å¹³å°ä¸Šã€‚'
        else:
            context = f'è¿™æ˜¯ä¸€ä¸ª SO åº“ï¼ˆ{so_file_name}ï¼‰ï¼Œæ¥è‡ª {Path(so_file).parent.name} ç›®å½•ã€‚'

        return context

    def _get_call_stack_info(self, file_path: str, address: str, vaddr: int) -> Optional[dict]:
        """ä» perf.db è·å–è°ƒç”¨å †æ ˆä¿¡æ¯

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            address: åœ°å€å­—ç¬¦ä¸²
            vaddr: è™šæ‹Ÿåœ°å€åç§»é‡

        Returns:
            è°ƒç”¨å †æ ˆä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«è°ƒç”¨è€…å’Œè¢«è°ƒç”¨è€…ä¿¡æ¯
        """
        if not self.perf_db_file or not self.perf_db_file.exists():
            return None

        try:
            conn = sqlite3.connect(str(self.perf_db_file))
            cursor = conn.cursor()

            try:
                # åŠ è½½æ˜ å°„å…³ç³»
                cursor.execute('SELECT DISTINCT file_id, path FROM perf_files WHERE path IS NOT NULL')
                file_id_to_path = {row[0]: row[1] for row in cursor.fetchall()}

                cursor.execute('SELECT id, data FROM data_dict WHERE data IS NOT NULL')
                name_to_data = {row[0]: row[1] for row in cursor.fetchall()}

                # æŸ¥æ‰¾æ–‡ä»¶ ID
                file_id = None
                for fid, path in file_id_to_path.items():
                    if path == file_path:
                        file_id = fid
                        break

                if file_id is None:
                    return None

                # æŸ¥æ‰¾åœ°å€ ID
                name_id = None
                for nid, data in name_to_data.items():
                    if data == address:
                        name_id = nid
                        break

                if name_id is None:
                    return None

                # æŸ¥è¯¢è°ƒç”¨å †æ ˆï¼šæ‰¾åˆ°è°ƒç”¨è¿™ä¸ªå‡½æ•°çš„å‡½æ•°ï¼ˆdepth æ›´å¤§çš„ï¼Œå› ä¸º depth è¶Šå¤§ä»£è¡¨æºå¤´/è°ƒç”¨è€…ï¼‰
                # ç›´æ¥åœ¨åŒä¸€ä¸ª callchain_id ä¸­æŸ¥æ‰¾ï¼Œä¸éœ€è¦ JOIN perf_sample
                cursor.execute(
                    """
                    SELECT DISTINCT pc2.file_id, pc2.name, pc2.depth, pc2.symbol_id as caller_symbol_id
                    FROM perf_callchain pc1
                    JOIN perf_callchain pc2 ON pc1.callchain_id = pc2.callchain_id
                    WHERE pc1.file_id = ? AND pc1.name = ? AND pc1.symbol_id = -1
                      AND pc2.depth > pc1.depth
                      AND pc2.symbol_id != -1
                    ORDER BY pc2.depth ASC
                    LIMIT 10
                """,
                    (file_id, name_id),
                )

                callers = []
                for row in cursor.fetchall():
                    caller_file_id, caller_name_id, caller_depth, caller_symbol_id = row
                    caller_file_path = file_id_to_path.get(caller_file_id, '')
                    caller_address = name_to_data.get(caller_name_id, '')

                    # è·å–è°ƒç”¨è€…å‡½æ•°åï¼ˆå¦‚æœæœ‰ç¬¦å·ï¼‰
                    caller_symbol_name = None
                    if caller_symbol_id and caller_symbol_id != -1:
                        try:
                            cursor.execute('SELECT name FROM perf_symbols WHERE id = ?', (caller_symbol_id,))
                            symbol_row = cursor.fetchone()
                            if symbol_row:
                                caller_symbol_name = symbol_row[0]
                        except sqlite3.OperationalError:
                            # perf_symbols è¡¨å¯èƒ½ä¸å­˜åœ¨ï¼Œå¿½ç•¥
                            pass

                    if caller_file_path and caller_address:
                        callers.append(
                            {
                                'file_path': caller_file_path,
                                'address': caller_address,
                                'symbol_name': caller_symbol_name,
                                'depth': caller_depth,
                            }
                        )

                # æŸ¥è¯¢è¢«è°ƒç”¨è€…ï¼šæ‰¾åˆ°è¿™ä¸ªå‡½æ•°è°ƒç”¨çš„å‡½æ•°ï¼ˆdepth æ›´å°çš„ï¼Œå› ä¸º depth è¶Šå°ä»£è¡¨è¢«è°ƒç”¨åˆ°çš„ï¼‰
                # ç›´æ¥åœ¨åŒä¸€ä¸ª callchain_id ä¸­æŸ¥æ‰¾ï¼Œä¸éœ€è¦ JOIN perf_sample
                cursor.execute(
                    """
                    SELECT DISTINCT pc2.file_id, pc2.name, pc2.depth, pc2.symbol_id as callee_symbol_id
                    FROM perf_callchain pc1
                    JOIN perf_callchain pc2 ON pc1.callchain_id = pc2.callchain_id
                    WHERE pc1.file_id = ? AND pc1.name = ? AND pc1.symbol_id = -1
                      AND pc2.depth < pc1.depth
                      AND pc2.symbol_id != -1
                    ORDER BY pc2.depth DESC
                    LIMIT 10
                """,
                    (file_id, name_id),
                )

                callees = []
                for row in cursor.fetchall():
                    callee_file_id, callee_name_id, callee_depth, callee_symbol_id = row
                    callee_file_path = file_id_to_path.get(callee_file_id, '')
                    callee_address = name_to_data.get(callee_name_id, '')

                    # è·å–è¢«è°ƒç”¨è€…å‡½æ•°åï¼ˆå¦‚æœæœ‰ç¬¦å·ï¼‰
                    callee_symbol_name = None
                    if callee_symbol_id and callee_symbol_id != -1:
                        try:
                            cursor.execute('SELECT name FROM perf_symbols WHERE id = ?', (callee_symbol_id,))
                            symbol_row = cursor.fetchone()
                            if symbol_row:
                                callee_symbol_name = symbol_row[0]
                        except sqlite3.OperationalError:
                            # perf_symbols è¡¨å¯èƒ½ä¸å­˜åœ¨ï¼Œå¿½ç•¥
                            pass

                    if callee_file_path and callee_address:
                        callees.append(
                            {
                                'file_path': callee_file_path,
                                'address': callee_address,
                                'symbol_name': callee_symbol_name,
                                'depth': callee_depth,
                            }
                        )

                return {
                    'callers': callers[:5],  # é™åˆ¶æœ€å¤š5ä¸ªè°ƒç”¨è€…
                    'callees': callees[:5],  # é™åˆ¶æœ€å¤š5ä¸ªè¢«è°ƒç”¨è€…
                }

            finally:
                conn.close()

        except Exception as e:
            logger.warning(f'Error getting call stack information: {e}')
            return None

    def _enhance_context_with_call_stack(self, base_context: str, call_stack_info: Optional[dict]) -> str:
        """å¢å¼ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ·»åŠ è°ƒç”¨å †æ ˆä¿¡æ¯

        Args:
            base_context: åŸºç¡€ä¸Šä¸‹æ–‡ä¿¡æ¯
            call_stack_info: è°ƒç”¨å †æ ˆä¿¡æ¯

        Returns:
            å¢å¼ºåçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        if not call_stack_info:
            return base_context

        context_parts = [base_context]

        # æ·»åŠ è°ƒç”¨è€…ä¿¡æ¯
        callers = call_stack_info.get('callers', [])
        if callers:
            context_parts.append('\nè°ƒç”¨å †æ ˆä¿¡æ¯ï¼ˆè°è°ƒç”¨äº†è¿™ä¸ªå‡½æ•°ï¼‰:')
            for i, caller in enumerate(callers[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
                caller_info = f'  {i}. '
                if caller.get('symbol_name'):
                    caller_info += f'{caller["symbol_name"]} '
                caller_info += f'({caller["file_path"]} {caller["address"]})'
                context_parts.append(caller_info)

        # æ·»åŠ è¢«è°ƒç”¨è€…ä¿¡æ¯ï¼ˆæœ‰ç¬¦å·çš„å‡½æ•°ï¼‰
        callees = call_stack_info.get('callees', [])
        if callees:
            context_parts.append('\nè¢«è°ƒç”¨çš„å‡½æ•°ï¼ˆè¿™ä¸ªå‡½æ•°è°ƒç”¨äº†å“ªäº›æœ‰ç¬¦å·çš„å‡½æ•°ï¼‰:')
            for i, callee in enumerate(callees[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
                callee_info = f'  {i}. '
                if callee.get('symbol_name'):
                    callee_info += f'{callee["symbol_name"]} '
                callee_info += f'({callee["file_path"]} {callee["address"]})'
                context_parts.append(callee_info)

        return '\n'.join(context_parts)

    def extract_strings_near_offset(self, elf_file, vaddr, range_size=200):
        """
        æå–è™šæ‹Ÿåœ°å€é™„è¿‘çš„å­—ç¬¦ä¸²å¸¸é‡ï¼ˆä½¿ç”¨ç²¾å‡†æå–ï¼‰

        æ³¨æ„ï¼šrange_size å‚æ•°ä¿ç•™ä»¥ä¿æŒæ¥å£å…¼å®¹æ€§ï¼Œä½†å®é™…ä½¿ç”¨ç²¾å‡†æå–é€»è¾‘
        """
        # åˆå§‹åŒ–å­—ç¬¦ä¸²æå–å™¨ï¼ˆå¦‚æœè¿˜æ²¡æœ‰åˆå§‹åŒ–ï¼‰
        self._init_string_extractor()

        # ä½¿ç”¨é€šç”¨çš„å­—ç¬¦ä¸²æå–å™¨
        return self.string_extractor.extract_strings_near_offset(elf_file, vaddr)

    def analyze_function(self, file_path, address, call_count, rank, event_count=None, skip_llm=False):
        """åˆ†æå•ä¸ªå‡½æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ radare2ï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨ capstoneï¼‰

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            address: åœ°å€
            call_count: è°ƒç”¨æ¬¡æ•°
            rank: æ’å
            event_count: äº‹ä»¶è®¡æ•°ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ä¼˜å…ˆæ˜¾ç¤ºï¼‰
        """
        logger.info(f'\n{"=" * 80}')
        logger.info(f'Analyzing function #{rank}: {file_path}')
        if event_count is not None and event_count > 0:
            logger.info(
                f'Address: {address}, instruction count (event_count): {event_count:,}, call count: {call_count:,}'
            )
        else:
            logger.info(f'Address: {address}, call count: {call_count:,}')
        logger.info(f'{"=" * 80}')

        # æå–è™šæ‹Ÿåœ°å€ï¼ˆåç§»é‡ï¼‰
        vaddr = self.extract_offset_from_address(address)
        if vaddr is None:
            logger.warning(' Unable to parse address: %s', address)
            return None

        # æ‰¾åˆ° SO æ–‡ä»¶
        # å¦‚æœæŒ‡å®šäº† --so-fileï¼Œä¸” address ä¸­åŒ…å« SO æ–‡ä»¶åï¼Œä¼˜å…ˆä½¿ç”¨ address ä¸­çš„ä¿¡æ¯
        so_file = None
        if self.so_dir and self.so_dir.is_file() and '+' in address:
            # ä» address ä¸­æå– SO æ–‡ä»¶åï¼ˆå¦‚ libquickjs.so+0x123 -> libquickjs.soï¼‰
            address_so_name = address.split('+')[0]
            # å¦‚æœ address ä¸­çš„ SO æ–‡ä»¶åä¸æŒ‡å®šçš„ SO æ–‡ä»¶åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨æŒ‡å®šçš„æ–‡ä»¶
            if address_so_name == self.so_dir.name:
                so_file = self.so_dir.resolve()

        # å¦‚æœè¿˜æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä» file_path æŸ¥æ‰¾
        if not so_file:
            so_file = self.find_so_file(file_path)

        if not so_file:
            logger.warning(' SO file not found: %s (address: %s)', file_path, address)
            return None

        logger.info(f'âœ… Found SO file: {so_file}')

        # ä¼˜å…ˆä½¿ç”¨ radare2ï¼ˆå¦‚æœå¯ç”¨ä¸”æœªå¼ºåˆ¶ä½¿ç”¨ Capstoneï¼‰
        if R2_AVAILABLE and not self.use_capstone_only:
            # å»¶è¿Ÿæ£€æµ‹ radare2 æ˜¯å¦çœŸçš„å¯ç”¨ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶æ£€æµ‹ï¼‰
            if self._r2_actually_available is None:
                self._r2_actually_available = _check_r2_actually_available()
                if not self._r2_actually_available:
                    logger.info('â„¹ï¸  radare2 command not in PATH, will use capstone for disassembly')

            # å¦‚æœ radare2 å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨å®ƒ
            if self._r2_actually_available:
                try:
                    return self._analyze_function_with_r2(
                        so_file,
                        vaddr,
                        file_path,
                        address,
                        call_count,
                        rank,
                        event_count,
                        skip_llm,
                    )
                except (FileNotFoundError, Exception) as e:
                    # å¦‚æœ radare2 ä¸å¯ç”¨ï¼ˆFileNotFoundError: ERROR: Cannot find radare2 in PATHï¼‰
                    # æ ‡è®°ä¸ºä¸å¯ç”¨ï¼Œåç»­ç›´æ¥ä½¿ç”¨ capstone
                    if isinstance(e, FileNotFoundError) and 'radare2' in str(e):
                        logger.warning(' radare2 command unavailable, automatically switching to capstone method')
                        self._r2_actually_available = False  # æ ‡è®°ä¸ºä¸å¯ç”¨ï¼Œé¿å…é‡å¤å°è¯•
                    else:
                        logger.exception(' radare2 åˆ†æå¤±è´¥ï¼Œå›é€€åˆ° capstone æ–¹æ³•')

        # ä½¿ç”¨ capstone æ–¹æ³•ï¼ˆå›é€€æ–¹æ¡ˆæˆ–é»˜è®¤æ–¹æ¡ˆï¼‰
        return self._analyze_function_with_capstone(
            so_file, vaddr, file_path, address, call_count, rank, event_count, skip_llm
        )

    def _analyze_function_with_r2(
        self,
        so_file,
        vaddr,
        file_path,
        address,
        call_count,
        rank,
        event_count=None,
        skip_llm=False,
    ):
        """ä½¿ç”¨ radare2 åˆ†æå‡½æ•°ï¼ˆä¼˜åŒ–ï¼šå¤ç”¨åŒä¸€ä¸ª SO æ–‡ä»¶çš„ radare2 å®ä¾‹ï¼‰"""
        logger.info('ğŸ”§ Using radare2 for function analysis')

        # ä¼˜åŒ–ï¼šå¤ç”¨åŒä¸€ä¸ª SO æ–‡ä»¶çš„ radare2 å®ä¾‹ï¼Œé¿å…é‡å¤è¿è¡Œ aaa
        # ç¡®ä¿è·¯å¾„è§£æä¸€è‡´ï¼šç»Ÿä¸€è½¬æ¢ä¸ºç»å¯¹è·¯å¾„å¹¶æ ‡å‡†åŒ–
        so_file_path_obj = Path(so_file).resolve() if isinstance(so_file, str) else so_file.resolve()

        # ä½¿ç”¨æ ‡å‡†åŒ–çš„ç»å¯¹è·¯å¾„ä½œä¸ºç¼“å­˜é”®ï¼ˆç¡®ä¿è·¯å¾„æ ¼å¼ä¸€è‡´ï¼‰
        # Windows ä¸Šè·¯å¾„ä¸åŒºåˆ†å¤§å°å†™ï¼Œç»Ÿä¸€è½¬æ¢ä¸ºå°å†™å¹¶æ ‡å‡†åŒ–è·¯å¾„åˆ†éš”ç¬¦
        if os.name == 'nt':
            # Windows: è½¬æ¢ä¸ºå°å†™ï¼Œç»Ÿä¸€ä½¿ç”¨æ­£æ–œæ ï¼Œå»é™¤å°¾éƒ¨æ–œæ 
            so_file_path = str(so_file_path_obj).lower().replace('\\', '/').rstrip('/')
        else:
            # Unix/Linux: ä¿æŒåŸæ ·ï¼Œä½†æ ‡å‡†åŒ–è·¯å¾„
            so_file_path = str(so_file_path_obj)

        # ç¡®ä¿ _r2_analyzers å·²åˆå§‹åŒ–
        if not hasattr(self, '_r2_analyzers'):
            self._r2_analyzers = {}

        # æ£€æŸ¥ç¼“å­˜ï¼ˆä½¿ç”¨è·¯å¾„å¯¹è±¡æ¯”è¾ƒï¼Œæ›´å¯é ï¼‰
        cache_match = False
        matched_key = None
        for cached_key in self._r2_analyzers:
            try:
                # å°†ç¼“å­˜é”®å’Œå½“å‰è·¯å¾„éƒ½è½¬æ¢ä¸º Path å¯¹è±¡å¹¶è§£æä¸ºç»å¯¹è·¯å¾„è¿›è¡Œæ¯”è¾ƒ
                cached_path = Path(cached_key).resolve()
                current_path = so_file_path_obj.resolve()
                # ä½¿ç”¨ Path å¯¹è±¡çš„æ¯”è¾ƒï¼ˆæ›´å¯é ï¼‰
                if cached_path == current_path:
                    cache_match = True
                    matched_key = cached_key
                    break
            except Exception:
                # å¦‚æœè·¯å¾„è§£æå¤±è´¥ï¼Œå›é€€åˆ°å­—ç¬¦ä¸²æ¯”è¾ƒ
                if os.name == 'nt':
                    cached_normalized = cached_key.lower().replace('\\', '/').rstrip('/')
                    current_normalized = so_file_path.lower().replace('\\', '/').rstrip('/')
                else:
                    cached_normalized = cached_key
                    current_normalized = so_file_path
                if cached_normalized == current_normalized:
                    cache_match = True
                    matched_key = cached_key
                    break

        if not cache_match:
            # ç¬¬ä¸€æ¬¡æ‰“å¼€è¯¥ SO æ–‡ä»¶ï¼Œåˆ›å»ºå¹¶ç¼“å­˜åˆ†æå™¨å®ä¾‹
            logger.info(f'ğŸ“‚ First time opening SO file, initializing radare2 analyzer: {so_file_path_obj.name}')
            r2_analyzer = R2FunctionAnalyzer(so_file_path_obj, skip_decompilation=self.skip_decompilation)
            r2_analyzer.__enter__()  # æ‰‹åŠ¨è¿›å…¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œä½†ä¸é€€å‡º
            self._r2_analyzers[so_file_path] = r2_analyzer
        else:
            # å¤ç”¨å·²å­˜åœ¨çš„åˆ†æå™¨å®ä¾‹
            r2_analyzer = self._r2_analyzers[matched_key]
            logger.info(f'â™»ï¸  Reusing radare2 analyzer instance: {so_file_path_obj.name} (cached)')

        # ä½¿ç”¨ç¼“å­˜çš„åˆ†æå™¨å®ä¾‹è¿›è¡Œåˆ†æ
        result = r2_analyzer.analyze_function_at_offset(vaddr)
        if not result:
            return None

        func_info = result['func_info']
        instructions_str = result['instructions']
        strings = result['strings']
        called_functions = result.get('called_functions', [])  # è·å–è¢«è°ƒç”¨çš„å‡½æ•°åˆ—è¡¨
        decompiled = result.get('decompiled')  # è·å–åç¼–è¯‘ä»£ç ï¼ˆå¦‚æœå¯ç”¨ï¼‰

        # è½¬æ¢æŒ‡ä»¤æ ¼å¼ï¼ˆä»å­—ç¬¦ä¸²è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œä¿æŒå…¼å®¹æ€§ï¼‰
        instructions = instructions_str  # å·²ç»æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨æ ¼å¼

        # è·å–è°ƒç”¨å †æ ˆä¿¡æ¯ï¼ˆä» perf.dbï¼‰
        call_stack_info = None
        if self.perf_db_file and self.perf_db_file.exists():
            try:
                call_stack_info = self._get_call_stack_info(file_path, address, vaddr)
            except Exception as e:
                logger.warning(f' Failed to get call stack information: {e}')

        # LLM åˆ†æï¼ˆå¦‚æœ skip_llm=Trueï¼Œåˆ™è·³è¿‡ï¼‰
        llm_result = None
        if self.use_llm and self.llm_analyzer and not skip_llm:
            logger.info('Analyzing function with LLM...')
            try:
                # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆä¸åŒ…å«è°ƒç”¨å †æ ˆä¿¡æ¯ï¼Œè°ƒç”¨å †æ ˆä¿¡æ¯åº”è¯¥åœ¨æ¯ä¸ªå‡½æ•°çš„ä¸Šä¸‹æ–‡ä¸­å•ç‹¬æ˜¾ç¤ºï¼‰
                base_context = self.context if self.context else self._build_context(so_file, file_path)
                context = base_context

                # è·å–å‡½æ•°åï¼ˆå¦‚æœæœ‰ï¼‰
                symbol_name = func_info.get('name', '')
                if symbol_name.startswith('sym.imp.') or symbol_name.startswith('fcn.'):
                    symbol_name = None  # è¿™äº›æ˜¯å¯¼å…¥å‡½æ•°æˆ–è‡ªåŠ¨ç”Ÿæˆçš„åç§°ï¼Œä¸ä½¿ç”¨

                llm_result = self.llm_analyzer.analyze_with_llm(
                    instructions=instructions,
                    strings=strings,
                    symbol_name=symbol_name,
                    called_functions=called_functions,  # ä¼ é€’è¢«è°ƒç”¨çš„å‡½æ•°åˆ—è¡¨
                    offset=vaddr,
                    context=context,
                    func_info=func_info,  # ä¼ é€’å‡½æ•°å…ƒä¿¡æ¯
                    call_count=call_count,  # ä¼ é€’è°ƒç”¨æ¬¡æ•°
                    event_count=event_count,  # ä¼ é€’æŒ‡ä»¤æ‰§è¡Œæ¬¡æ•°
                    so_file=str(so_file_path) if so_file_path else None,  # ä¼ é€’ SO æ–‡ä»¶è·¯å¾„
                )

                logger.info('âœ… LLM analysis completed')
                logger.info(f'   Inferred function name: {llm_result.get("function_name", "N/A")}')
                logger.info(f'   Functionality description: {llm_result.get("functionality", "N/A")[:100]}...')
                logger.info(f'   Confidence: {llm_result.get("confidence", "N/A")}')
            except Exception:
                logger.exception('âŒ LLM åˆ†æå¤±è´¥')

        # è¿”å›ç»“æœï¼ˆä¿æŒä¸ capstone æ–¹æ³•ç›¸åŒçš„æ ¼å¼ï¼‰
        return {
            'rank': rank,
            'file_path': file_path,
            'address': address,
            'offset': f'0x{vaddr:x}',
            'call_count': call_count,
            'event_count': event_count,  # æ·»åŠ  event_count
            'so_file': str(so_file),
            'instruction_count': len(instructions),
            'instructions': instructions,  # å­—ç¬¦ä¸²åˆ—è¡¨æ ¼å¼
            'strings': ', '.join(strings[:5]) if strings else '',
            'called_functions': called_functions,  # æ·»åŠ è¢«è°ƒç”¨çš„å‡½æ•°åˆ—è¡¨
            'decompiled': decompiled,  # æ·»åŠ åç¼–è¯‘ä»£ç ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            'llm_result': llm_result,
            'func_info': func_info,  # é¢å¤–çš„å‡½æ•°ä¿¡æ¯
            'call_stack_info': call_stack_info,  # æ·»åŠ è°ƒç”¨å †æ ˆä¿¡æ¯
        }

    def _analyze_function_with_capstone(
        self,
        so_file,
        vaddr,
        file_path,
        address,
        call_count,
        rank,
        event_count=None,
        skip_llm=False,
    ):
        """ä½¿ç”¨ capstone åˆ†æå‡½æ•°ï¼ˆåŸæœ‰æ–¹æ³•ï¼‰"""
        logger.info('ğŸ”§ Using capstone for function analysis')

        # æ‰“å¼€ ELF æ–‡ä»¶
        try:
            with open(so_file, 'rb') as f:
                elf_file = ELFFile(f)

                # åæ±‡ç¼–å‡½æ•°
                logger.info(f'Disassembling (vaddr=0x{vaddr:x})...')
                instructions = self.disassemble_function(elf_file, vaddr, size=2000)

                if not instructions:
                    logger.error('âŒ Disassembly failed')
                    return None

                logger.info(f'âœ… Disassembly successful, {len(instructions)} instructions')

                # åˆå§‹åŒ–å­—ç¬¦ä¸²æå–å™¨ï¼ˆå¦‚æœè¿˜æ²¡æœ‰åˆå§‹åŒ–ï¼‰
                self._init_string_extractor()

                # æå–å­—ç¬¦ä¸²ï¼ˆä¼ å…¥æŒ‡ä»¤åˆ—è¡¨ä»¥è¿›è¡Œç²¾å‡†åˆ†æï¼‰
                strings = []
                try:
                    extracted_strings = self.string_extractor.extract_strings_from_instructions(
                        elf_file, instructions, vaddr
                    )
                    if extracted_strings:
                        strings = extracted_strings
                        logger.info(
                            f'æ‰¾åˆ° {len(strings)} ä¸ªå­—ç¬¦ä¸²å¸¸é‡: {", ".join(strings[:3])}{"..." if len(strings) > 3 else ""}'
                        )
                    else:
                        logger.warning(' No string constants found')
                        # å¦‚æœç²¾å‡†æå–æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯• fallbackï¼ˆä» .rodata æ®µæå–ï¼‰
                        fallback_strings = self.string_extractor._fallback_extract_strings(elf_file)
                        if fallback_strings:
                            strings = fallback_strings
                            logger.info(
                                f'ä½¿ç”¨ fallback æ–¹æ³•æ‰¾åˆ° {len(strings)} ä¸ªå­—ç¬¦ä¸²å¸¸é‡: {", ".join(strings[:3])}{"..." if len(strings) > 3 else ""}'
                            )
                except Exception:
                    logger.exception(' å­—ç¬¦ä¸²æå–å¤±è´¥')
                    strings = []  # ç¡®ä¿ strings æœ‰é»˜è®¤å€¼

                # LLM åˆ†æï¼ˆå¦‚æœ skip_llm=Trueï¼Œåˆ™è·³è¿‡ï¼‰
                llm_result = None
                if self.use_llm and self.llm_analyzer and not skip_llm:
                    logger.info('Analyzing function with LLM...')
                    try:
                        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚æœæä¾›äº†è‡ªå®šä¹‰ä¸Šä¸‹æ–‡åˆ™ä½¿ç”¨ï¼Œå¦åˆ™æ ¹æ® SO æ–‡ä»¶è‡ªåŠ¨æ¨æ–­ï¼‰
                        context = self.context if self.context else self._build_context(so_file, file_path)

                        llm_result = self.llm_analyzer.analyze_with_llm(
                            instructions=[f'{inst.address:x}: {inst.mnemonic} {inst.op_str}' for inst in instructions],
                            strings=strings,
                            symbol_name=None,
                            called_functions=[],
                            offset=vaddr,
                            context=context,
                            func_info=None,  # capstone æ¨¡å¼ä¸‹æ²¡æœ‰ func_info
                            call_count=call_count,
                            event_count=event_count,
                            so_file=str(so_file) if so_file else None,
                        )

                        logger.info('âœ… LLM analysis completed')
                        logger.info(f'   Inferred function name: {llm_result.get("function_name", "N/A")}')
                        logger.info(f'   Functionality description: {llm_result.get("functionality", "N/A")[:100]}...')
                        logger.info(f'   Confidence: {llm_result.get("confidence", "N/A")}')
                    except Exception:
                        logger.exception('âŒ LLM åˆ†æå¤±è´¥')

                # è¿”å›ç»“æœ
                # è·å–è°ƒç”¨å †æ ˆä¿¡æ¯ï¼ˆä» perf.dbï¼‰
                call_stack_info = None
                if self.perf_db_file and self.perf_db_file.exists():
                    try:
                        call_stack_info = self._get_call_stack_info(file_path, address, vaddr)
                    except Exception as e:
                        logger.warning(f' Failed to get call stack information: {e}')

                return {
                    'rank': rank,
                    'file_path': file_path,
                    'address': address,
                    'offset': f'0x{vaddr:x}',
                    'call_count': call_count,
                    'event_count': event_count,  # æ·»åŠ  event_count
                    'so_file': str(so_file),
                    'instruction_count': len(instructions),
                    'instructions': instructions,  # ä¿å­˜æŒ‡ä»¤ç”¨äº HTML æŠ¥å‘Š
                    'strings': ', '.join(strings[:5]) if strings else '',
                    'called_functions': [],  # capstone æ–¹æ³•æš‚ä¸æ”¯æŒè·å–è°ƒç”¨å‡½æ•°åˆ—è¡¨
                    'llm_result': llm_result,
                    'call_stack_info': call_stack_info,  # æ·»åŠ è°ƒç”¨å †æ ˆä¿¡æ¯
                }
        except Exception:
            logger.exception('âŒ åˆ†æå¤±è´¥')
            return None

    def _get_missing_symbols_from_perf_db(self, top_n=None):
        """ä» perf.db ä¸­æå–ç¼ºå¤±ç¬¦å·ï¼ˆcall_count æ¨¡å¼ï¼‰"""
        if top_n is None:
            top_n = DEFAULT_TOP_N

        logger.info('=' * 80)
        logger.info('âš¡ï¸ Extracting missing symbols from perf.db (call_count mode)')
        logger.info('=' * 80)

        conn = sqlite3.connect(str(self.perf_db_file))
        cursor = conn.cursor()

        try:
            # 1. åŠ è½½æ˜ å°„å…³ç³»
            logger.info('\nStep 1: Loading mappings...')
            cursor.execute('SELECT DISTINCT file_id, path FROM perf_files WHERE path IS NOT NULL')
            file_id_to_path = {row[0]: row[1] for row in cursor.fetchall()}
            logger.info(f'âœ… Loaded {len(file_id_to_path):,} file path mappings')

            cursor.execute('SELECT id, data FROM data_dict WHERE data IS NOT NULL')
            name_to_data = {row[0]: row[1] for row in cursor.fetchall()}
            logger.info(f'âœ… Loaded {len(name_to_data):,} address data mappings')

            # 2. æŸ¥è¯¢ç¼ºå¤±ç¬¦å·è®°å½•å¹¶èšåˆ
            logger.info('\nStep 2: Querying missing symbol records and aggregating...')
            cursor.execute("""
                SELECT file_id, name, ip, depth
                FROM perf_callchain
                WHERE symbol_id = -1
            """)

            address_call_counts = defaultdict(int)
            address_info = {}

            batch_size = 100000
            total_rows = 0
            while True:
                rows = cursor.fetchmany(batch_size)
                if not rows:
                    break

                total_rows += len(rows)
                for file_id, name_id, _ip, _depth in rows:
                    file_path = file_id_to_path.get(file_id, 'æœªçŸ¥æ–‡ä»¶')
                    address_data = name_to_data.get(name_id)

                    if address_data and file_path != 'æœªçŸ¥æ–‡ä»¶':
                        key = (file_path, address_data)
                        address_call_counts[key] += 1

                        if key not in address_info:
                            address_info[key] = {
                                'file_path': file_path,
                                'address': address_data,
                            }

            logger.info(
                f'âœ… Processing completed, {total_rows:,} records aggregated into {len(address_call_counts):,} unique addresses'
            )

            # 3. è¿‡æ»¤ç³»ç»Ÿæ–‡ä»¶ï¼Œå¹¶æ£€æµ‹ HAP åœ°å€
            excluded_exact = ['[shmm]', 'æœªçŸ¥æ–‡ä»¶', '/bin/devhost.elf']
            excluded_prefixes = ['/system', '/vendor/lib64', '/lib', '/chip_prod']

            filtered_data = []
            hap_addresses = []  # æ”¶é›† HAP åœ°å€ç”¨äºæ‰¹é‡è§£æ

            for (file_path, address), call_count in address_call_counts.items():
                if file_path in excluded_exact:
                    continue
                if any(file_path.startswith(prefix) for prefix in excluded_prefixes):
                    continue

                # æ£€æµ‹ HAP åœ°å€
                if HAP_RESOLVER_AVAILABLE and is_hap_address(address):
                    hap_addresses.append(address)
                    logger.debug(f'Detected HAP address: {address}')

                filtered_data.append(
                    {
                        'file_path': file_path,
                        'address': address,
                        'call_count': call_count,
                    }
                )

            # 4. æ‰¹é‡è§£æ HAP åœ°å€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            hap_resolutions = {}
            if hap_addresses and self.perf_db_file and HAP_RESOLVER_AVAILABLE:
                logger.info(f'\nStep 3: Batch resolving {len(hap_addresses)} HAP addresses...')
                hap_resolutions = resolve_hap_addresses_batch(self.perf_db_file, hap_addresses, quick_mode=True)

                # æ›´æ–° filtered_data ä¸­çš„ HAP åœ°å€ä¿¡æ¯
                for item in filtered_data:
                    address = item['address']
                    if address in hap_resolutions:
                        resolution = hap_resolutions[address]
                        if resolution.get('resolved') and resolution.get('so_file_path'):
                            # æ›´æ–°ä¸º SO æ–‡ä»¶è·¯å¾„å’Œåœ°å€
                            item['file_path'] = resolution['so_file_path']
                            item['address'] = f'{resolution["so_name"]}+0x{resolution["so_offset"]:x}'
                            item['hap_resolution'] = resolution
                            logger.info(f'  âœ… {address} -> {item["address"]}')
                        else:
                            item['hap_resolution'] = resolution
                            logger.warning(f'   {address} SO file not found')

            # 5. å¦‚æœæŒ‡å®šäº† --so-fileï¼Œè¿‡æ»¤åœ°å€ï¼ˆåªä¿ç•™æŒ‡å®š SO æ–‡ä»¶çš„åœ°å€ï¼‰
            if self.so_dir and self.so_dir.is_file():
                so_filtered_data = []
                for item in filtered_data:
                    address = item.get('address', '')
                    # ä» address ä¸­æå– SO æ–‡ä»¶åï¼ˆå¦‚ libquickjs.so+0x123 -> libquickjs.soï¼‰
                    if address and '+' in str(address):
                        address_so_name = str(address).split('+')[0]
                        if address_so_name == self.so_dir.name:
                            so_filtered_data.append(item)
                        else:
                            logger.debug(f'è·³è¿‡ä¸åŒ¹é…çš„åœ°å€: {address} (æœŸæœ›: {self.so_dir.name})')
                    elif not address or '+' not in str(address):
                        # å¦‚æœåœ°å€æ ¼å¼ä¸æ­£ç¡®ï¼Œä¹Ÿè·³è¿‡
                        logger.debug(f'è·³è¿‡æ ¼å¼ä¸æ­£ç¡®çš„åœ°å€: {address}')
                filtered_data = so_filtered_data
                logger.info(f'âœ… è¿‡æ»¤åå‰©ä½™ {len(filtered_data):,} ä¸ªåœ°å€ï¼ˆæŒ‡å®š SO æ–‡ä»¶: {self.so_dir.name}ï¼‰')

            # 6. æŒ‰è°ƒç”¨æ¬¡æ•°æ’åºï¼Œå–å‰ N ä¸ª
            sorted_data = sorted(filtered_data, key=lambda x: x['call_count'], reverse=True)[:top_n]
            logger.info(f'âœ… After filtering, {len(filtered_data):,} records remaining, selecting top {top_n}')

            # ç»Ÿè®¡ HAP åœ°å€è§£ææƒ…å†µ
            hap_count = sum(1 for item in sorted_data if 'hap_resolution' in item)
            if hap_count > 0:
                resolved_count = sum(1 for item in sorted_data if item.get('hap_resolution', {}).get('resolved'))
                logger.info(f'ğŸ“Š HAP address statistics: {hap_count} total, {resolved_count} successfully resolved')

            return sorted_data

        finally:
            conn.close()

    def analyze_top_functions(self, top_n=None):
        """åˆ†æå‰ N ä¸ªå‡½æ•°"""
        if top_n is None:
            top_n = DEFAULT_TOP_N
        logger.info('=' * 80)
        logger.info(f'Analyzing top {top_n} functions with missing symbols (by call count)')
        logger.info('=' * 80)

        # å¦‚æœæä¾›äº† excel_fileï¼Œä¼˜å…ˆä» Excel æ–‡ä»¶è¯»å–ï¼ˆå› ä¸º Excel æ–‡ä»¶å¯èƒ½å·²ç»æŒ‰ event_count æ’åºï¼‰
        if self.excel_file and self.excel_file.exists():
            df = pd.read_excel(self.excel_file)
            logger.info(f'\nReading Excel file: {len(df)} records')

            # æ£€æŸ¥æ˜¯å¦æœ‰ event_count åˆ—ï¼Œå¦‚æœæœ‰åˆ™æŒ‰ event_count æ’åºï¼Œå¦åˆ™æŒ‰è°ƒç”¨æ¬¡æ•°æ’åº
            if 'æŒ‡ä»¤æ•°(event_count)' in df.columns:
                # æŒ‰ event_count æ’åº
                top_df = df.nlargest(top_n, 'æŒ‡ä»¤æ•°(event_count)')
                logger.info(f'Selecting top {top_n} functions (sorted by event_count)')
            elif 'è°ƒç”¨æ¬¡æ•°' in df.columns:
                # æŒ‰è°ƒç”¨æ¬¡æ•°æ’åº
                top_df = df.nlargest(top_n, 'è°ƒç”¨æ¬¡æ•°')
                logger.info(f'Selecting top {top_n} functions (sorted by call count)')
            else:
                # å¦‚æœæ²¡æœ‰æ’åºåˆ—ï¼Œç›´æ¥å–å‰ top_n è¡Œï¼ˆä¿æŒåŸæœ‰é¡ºåºï¼‰
                top_df = df.head(top_n)
                logger.info(f'Selecting top {top_n} functions (keeping original order)')

            # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            data_list = []
            logger.info(f'ä» Excel è¯»å– {len(top_df)} è¡Œæ•°æ®ï¼Œå¼€å§‹è¿‡æ»¤...')
            for _, row in top_df.iterrows():
                # å¦‚æœæŒ‡å®šäº† --so-fileï¼Œè¿‡æ»¤åœ°å€ï¼ˆä» Excel è¯»å–æ—¶ä¹Ÿéœ€è¦è¿‡æ»¤ï¼‰
                if self.so_dir and self.so_dir.is_file():
                    address = row.get('åœ°å€', '')
                    # ä» address ä¸­æå– SO æ–‡ä»¶åï¼ˆå¦‚ libquickjs.so+0x123 -> libquickjs.soï¼‰
                    if address and '+' in str(address):
                        address_so_name = str(address).split('+')[0]
                        # å¦‚æœ address ä¸­çš„ SO æ–‡ä»¶åä¸æŒ‡å®šçš„ SO æ–‡ä»¶ä¸åŒ¹é…ï¼Œè·³è¿‡
                        if address_so_name != self.so_dir.name:
                            logger.warning(f'âš ï¸  ä» Excel è¯»å–æ—¶è·³è¿‡ä¸åŒ¹é…çš„åœ°å€: {address} (æœŸæœ›: {self.so_dir.name})')
                            continue
                    elif not address or '+' not in str(address):
                        # å¦‚æœåœ°å€æ ¼å¼ä¸æ­£ç¡®ï¼Œä¹Ÿè·³è¿‡
                        logger.warning(f'âš ï¸  ä» Excel è¯»å–æ—¶è·³è¿‡æ ¼å¼ä¸æ­£ç¡®çš„åœ°å€: {address}')
                        continue
                else:
                    # å¦‚æœæ²¡æœ‰æŒ‡å®š --so-fileï¼Œè®°å½•æ‰€æœ‰åœ°å€
                    address = row.get('åœ°å€', '')
                    logger.debug(f'ä» Excel è¯»å–åœ°å€: {address}')

                # è¯»å– event_countï¼Œå¤„ç† NaN å€¼
                event_count = None
                if 'æŒ‡ä»¤æ•°(event_count)' in row:
                    event_count_val = row['æŒ‡ä»¤æ•°(event_count)']
                    # å¤„ç† pandas çš„ NaN å€¼
                    if pd.notna(event_count_val) and event_count_val > 0:
                        event_count = int(event_count_val)
                    else:
                        # å¦‚æœ event_count æ˜¯ NaN æˆ– 0ï¼Œè®°å½•è­¦å‘Š
                        logger.warning(f'âš ï¸  åœ°å€ {row.get("åœ°å€", "unknown")} çš„ event_count ä¸º NaN æˆ– 0')

                data_list.append(
                    {
                        'file_path': row['æ–‡ä»¶è·¯å¾„'],
                        'address': row['åœ°å€'],
                        'call_count': row.get('è°ƒç”¨æ¬¡æ•°', 0),  # ä½¿ç”¨ get é¿å… KeyError
                        'event_count': event_count,  # å¦‚æœå­˜åœ¨ä¸”æœ‰æ•ˆï¼Œåˆ™ä½¿ç”¨
                    }
                )
        elif self.perf_db_file:
            # å¦åˆ™ä»æ•°æ®åº“è¯»å–
            data_list = self._get_missing_symbols_from_perf_db(top_n)

        # åˆ†ææ¯ä¸ªå‡½æ•°
        # å¦‚æœä½¿ç”¨æ‰¹é‡ LLM åˆ†æï¼Œå…ˆæ”¶é›†æ‰€æœ‰å‡½æ•°ä¿¡æ¯ï¼Œç„¶åæ‰¹é‡åˆ†æ
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ‰¹é‡åˆ†æå™¨
        is_batch_analyzer = (
            self.use_llm
            and self.use_batch_llm
            and self.llm_analyzer
            and isinstance(self.llm_analyzer, BatchLLMFunctionAnalyzer)
        )

        if is_batch_analyzer:
            # æ‰¹é‡åˆ†ææ¨¡å¼ï¼šå…ˆæ”¶é›†æ‰€æœ‰å‡½æ•°ä¿¡æ¯ï¼Œç„¶åæ‰¹é‡åˆ†æ
            logger.info(f'\nUsing batch LLM analysis mode (batch_size={self.batch_size})')
            results = []
            functions_data = []  # ç”¨äºæ‰¹é‡åˆ†æçš„æ•°æ®

            # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰å‡½æ•°çš„åæ±‡ç¼–å’Œå­—ç¬¦ä¸²ä¿¡æ¯ï¼ˆä¸è¿›è¡Œ LLM åˆ†æï¼‰
            logger.info('Step 1: Collecting disassembly and string information for all functions...')
            # ç¡®ä¿ _r2_analyzers å·²åˆå§‹åŒ–
            if not hasattr(self, '_r2_analyzers'):
                self._r2_analyzers = {}

            for idx, item in enumerate(data_list, 1):
                file_path = item['file_path']
                address = item['address']
                call_count = item.get('call_count', 0)
                event_count = item.get('event_count', None)
                rank = idx

                # å¤„ç† HAP åœ°å€ï¼ˆå¦‚æœä¹‹å‰æ²¡æœ‰è§£ææˆåŠŸï¼Œå†æ¬¡å°è¯•ï¼‰
                if HAP_RESOLVER_AVAILABLE and is_hap_address(address) and self.perf_db_file:
                    hap_resolution = item.get('hap_resolution')
                    if not hap_resolution or not hap_resolution.get('resolved'):
                        logger.info(f'ğŸ”„ Re-resolving HAP address: {address}')
                        resolution = resolve_hap_address_from_perfdb(
                            self.perf_db_file, address, quick_mode=True, so_dir=self.so_dir
                        )
                        if resolution and resolution.get('resolved'):
                            file_path = resolution['so_file_path']
                            address = f'{resolution["so_name"]}+0x{resolution["so_offset"]:x}'
                            logger.info(f'  âœ… Resolution successful: {address}')
                        else:
                            logger.warning(f'   Unable to resolve HAP address, skipping: {address}')
                            continue

                # åªè¿›è¡Œåæ±‡ç¼–å’Œå­—ç¬¦ä¸²æå–ï¼Œä¸è¿›è¡Œ LLM åˆ†æ
                result = self.analyze_function(file_path, address, call_count, rank, event_count, skip_llm=True)
                if result:
                    results.append(result)
                    # ç¡®ä¿è°ƒç”¨å †æ ˆä¿¡æ¯è¢«æ­£ç¡®ä¼ é€’
                    call_stack_info = result.get('call_stack_info')
                    if call_stack_info:
                        logger.debug(
                            f'å‡½æ•° #{rank} å·²æœ‰è°ƒç”¨å †æ ˆä¿¡æ¯: {len(call_stack_info.get("callers", []))} ä¸ªè°ƒç”¨è€…, {len(call_stack_info.get("callees", []))} ä¸ªè¢«è°ƒç”¨è€…'
                        )
                    else:
                        logger.debug(f'å‡½æ•° #{rank} æ²¡æœ‰è°ƒç”¨å †æ ˆä¿¡æ¯ï¼Œå°†åœ¨æ‰¹é‡åˆ†ææ—¶é‡æ–°è·å–')
                    # å‡†å¤‡æ‰¹é‡åˆ†æçš„æ•°æ®
                    instructions = result.get('instructions', [])
                    if isinstance(instructions, list) and len(instructions) > 0:
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦‚æœæ˜¯ Instruction å¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        if isinstance(instructions[0], str):
                            inst_list = instructions
                        else:
                            inst_list = [f'{inst.address:x}: {inst.mnemonic} {inst.op_str}' for inst in instructions]
                    else:
                        inst_list = []
                        # å¦‚æœæ²¡æœ‰æŒ‡ä»¤ï¼Œè®°å½•è­¦å‘Šä½†ä»ç„¶æ·»åŠ åˆ°æ‰¹é‡åˆ†æä¸­ï¼ˆä½¿ç”¨ç©ºæŒ‡ä»¤åˆ—è¡¨ï¼‰
                        logger.warning(
                            f' Function {rank} (address: {address}) has no instructions, will use empty instruction list for LLM analysis'
                        )

                    # å¤„ç†å­—ç¬¦ä¸²ï¼šå¦‚æœæ˜¯é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
                    strings_value = result.get('strings', '')
                    logger.debug(
                        f'Function #{rank} strings_value from result: {repr(strings_value)[:100]}, type: {type(strings_value)}'
                    )
                    if isinstance(strings_value, str):
                        strings_list = (
                            [s.strip() for s in strings_value.split(',') if s.strip()] if strings_value else []
                        )
                    else:
                        strings_list = strings_value if strings_value else []
                    logger.debug(f'Function #{rank} strings_list after processing: {len(strings_list)} items')

                    # ç¡®ä¿æ‰€æœ‰å‡½æ•°éƒ½è¢«æ·»åŠ åˆ°æ‰¹é‡åˆ†æä¸­ï¼Œå³ä½¿æŒ‡ä»¤ä¸ºç©º
                    # æ³¨æ„ï¼šoffset å­—æ®µåº”è¯¥ä½¿ç”¨åŸå§‹åœ°å€å­—ç¬¦ä¸²ï¼ˆå¦‚ "libxxx.so+0x123456"ï¼‰ï¼Œè¿™æ ·åœ¨ prompt ä¸­èƒ½æ­£ç¡®æ˜¾ç¤ºåŸå§‹åœ°å€
                    # result.get('offset') è¿”å›çš„æ˜¯ '0x...' æ ¼å¼ï¼Œä½†æˆ‘ä»¬éœ€è¦åŸå§‹åœ°å€å­—ç¬¦ä¸²
                    offset_value = address  # ä½¿ç”¨åŸå§‹åœ°å€å­—ç¬¦ä¸²ï¼ˆå¦‚ "libquick.so+0xc8ef4"ï¼‰ï¼Œè€Œä¸æ˜¯è§£æåçš„åç§»é‡

                    call_stack_info = result.get('call_stack_info')
                    # å¦‚æœæ²¡æœ‰è°ƒç”¨å †æ ˆä¿¡æ¯ï¼Œå°è¯•é‡æ–°è·å–
                    if not call_stack_info and self.perf_db_file and self.perf_db_file.exists():
                        try:
                            # ä»åœ°å€ä¸­æå–åç§»é‡
                            if '+' in address:
                                offset_str = address.split('+', 1)[1]
                                try:
                                    vaddr = (
                                        int(offset_str, 16) if not offset_str.startswith('0x') else int(offset_str, 16)
                                    )
                                    call_stack_info = self._get_call_stack_info(file_path, address, vaddr)
                                    if call_stack_info:
                                        logger.info(
                                            f'å‡½æ•° #{rank} é‡æ–°è·å–è°ƒç”¨å †æ ˆä¿¡æ¯æˆåŠŸ: {len(call_stack_info.get("callers", []))} ä¸ªè°ƒç”¨è€…, {len(call_stack_info.get("callees", []))} ä¸ªè¢«è°ƒç”¨è€…'
                                        )
                                except (ValueError, Exception) as e:
                                    logger.debug(f'å‡½æ•° #{rank} æ— æ³•è§£æåœ°å€ {address}: {e}')
                        except Exception as e:
                            logger.debug(f'å‡½æ•° #{rank} è·å–è°ƒç”¨å †æ ˆä¿¡æ¯å¤±è´¥: {e}')

                    functions_data.append(
                        {
                            'function_id': f'func_{rank}',
                            'offset': offset_value,  # ä½¿ç”¨åŸå§‹åœ°å€å­—ç¬¦ä¸²ï¼Œç¡®ä¿ prompt ä¸­æ˜¾ç¤ºæ­£ç¡®çš„åœ°å€
                            'instructions': inst_list,  # å³ä½¿ä¸ºç©ºä¹Ÿæ·»åŠ 
                            'strings': strings_list,
                            'symbol_name': None,
                            'called_functions': result.get('called_functions', []),
                            'decompiled': result.get('decompiled'),  # æ·»åŠ åç¼–è¯‘ä»£ç 
                            'func_info': result.get('func_info'),  # æ·»åŠ å‡½æ•°å…ƒä¿¡æ¯
                            'call_count': result.get('call_count', 0),  # æ·»åŠ è°ƒç”¨æ¬¡æ•°
                            'event_count': result.get('event_count', 0),  # æ·»åŠ æŒ‡ä»¤æ‰§è¡Œæ¬¡æ•°
                            'so_file': result.get('so_file'),  # æ·»åŠ  SO æ–‡ä»¶è·¯å¾„
                            'rank': rank,
                            'file_path': file_path,
                            'address': address,  # ä¿ç•™åŸå§‹åœ°å€å­—ç¬¦ä¸²
                            'call_stack_info': call_stack_info,  # æ·»åŠ è°ƒç”¨å †æ ˆä¿¡æ¯
                        }
                    )
                    logger.debug(
                        f'Function #{rank} added to functions_data: strings={len(strings_list)}, called_functions={len(result.get("called_functions", []))}, call_stack_info={call_stack_info is not None}'
                    )
                    logger.debug(
                        f'âœ… å‡½æ•° #{rank} ({address}) å·²æ·»åŠ åˆ° functions_dataï¼ŒæŒ‡ä»¤æ•°: {len(inst_list)}, åç¼–è¯‘: {"æœ‰" if result.get("decompiled") else "æ— "}'
                    )

                    # æ¯10ä¸ªå‡½æ•°æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    if len(results) % 10 == 0:
                        logger.info(f'  Progress: {len(results)}/{top_n} function information collection completed')

            # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡ LLM åˆ†æï¼ˆæŒ‰ SO æ–‡ä»¶åˆ†ç»„ï¼‰
            if functions_data and self.llm_analyzer:
                logger.info(f'\nStep 2: Batch LLM analysis of {len(functions_data)} functions...')
                logger.debug(
                    f'functions_data é•¿åº¦: {len(functions_data)}, llm_analyzer: {self.llm_analyzer is not None}'
                )

                # æŒ‰ SO æ–‡ä»¶åˆ†ç»„
                functions_by_so = {}
                for func_data in functions_data:
                    # ä» func_data ä¸­æå– SO æ–‡ä»¶å
                    so_file_path = func_data.get('so_file')
                    if not so_file_path:
                        # å¦‚æœæ²¡æœ‰ so_fileï¼Œå°è¯•ä» file_path æˆ– address ä¸­æå–
                        file_path = func_data.get('file_path', '')
                        address = func_data.get('address', '')
                        # ä»åœ°å€ä¸­æå– SO æ–‡ä»¶åï¼ˆå¦‚ libquickjs.so+0x123 -> libquickjs.soï¼‰
                        if '+' in address:
                            so_name = address.split('+')[0]
                        elif '/' in file_path:
                            so_name = file_path.split('/')[-1]
                        else:
                            so_name = 'unknown'
                    else:
                        # ä» SO æ–‡ä»¶è·¯å¾„ä¸­æå–æ–‡ä»¶å
                        so_name = Path(so_file_path).name if so_file_path else 'unknown'

                    if so_name not in functions_by_so:
                        functions_by_so[so_name] = []
                    functions_by_so[so_name].append(func_data)

                logger.info(f'æŒ‰ SO æ–‡ä»¶åˆ†ç»„: {len(functions_by_so)} ä¸ªä¸åŒçš„ SO æ–‡ä»¶')
                for so_name, funcs in functions_by_so.items():
                    logger.info(f'  - {so_name}: {len(funcs)} ä¸ªå‡½æ•°')

                # å¯¹æ¯ä¸ª SO æ–‡ä»¶ç»„åˆ†åˆ«è¿›è¡Œæ‰¹é‡åˆ†æ
                all_batch_results = []
                for so_name, so_functions in functions_by_so.items():
                    logger.info(f'\nåˆ†æ {so_name} ä¸­çš„ {len(so_functions)} ä¸ªå‡½æ•°...')

                    # ä¸ºè¿™ä¸ª SO æ–‡ä»¶ç»„æ„å»ºä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è°ƒç”¨å †æ ˆä¿¡æ¯ï¼‰
                    context = None
                    if so_functions:
                        # ä½¿ç”¨è¯¥ç»„ä¸­ç¬¬ä¸€ä¸ªå‡½æ•°æ¥æ„å»ºä¸Šä¸‹æ–‡
                        first_func = so_functions[0]
                        so_file_path = first_func.get('so_file')
                        file_path = first_func.get('file_path', '')
                        address = first_func.get('address', '')

                        if so_file_path:
                            so_file = Path(so_file_path)
                            base_context = self.context if self.context else self._build_context(so_file, file_path)
                        elif file_path:
                            # å¦‚æœæ²¡æœ‰ so_fileï¼Œå°è¯•ä» file_path æŸ¥æ‰¾
                            so_file = self.find_so_file(file_path)
                            if so_file:
                                base_context = self.context if self.context else self._build_context(so_file, file_path)
                            else:
                                base_context = self.context if self.context else ''
                        else:
                            base_context = self.context if self.context else ''

                        # ç¡®ä¿æ¯ä¸ªå‡½æ•°éƒ½æœ‰è°ƒç”¨å †æ ˆä¿¡æ¯ï¼ˆå¦‚æœç¼ºå¤±åˆ™é‡æ–°è·å–ï¼‰
                        # æ³¨æ„ï¼šè°ƒç”¨å †æ ˆä¿¡æ¯æ˜¯æ¯ä¸ªåœ°å€ç‰¹æœ‰çš„ï¼Œä¸åº”è¯¥åˆå¹¶åˆ°æ•´ä½“èƒŒæ™¯ä¿¡æ¯ä¸­
                        # æ¯ä¸ªå‡½æ•°çš„è°ƒç”¨å †æ ˆä¿¡æ¯ä¼šåœ¨ batch_analyzer ä¸­å•ç‹¬æ˜¾ç¤º
                        for func in so_functions:
                            call_stack_info = func.get('call_stack_info')
                            func_file_path = func.get('file_path', '')
                            func_address = func.get('address', '')

                            if (
                                not call_stack_info
                                and self.perf_db_file
                                and self.perf_db_file.exists()
                                and func_file_path
                                and func_address
                            ):
                                try:
                                    # ä»åœ°å€ä¸­æå–åç§»é‡
                                    if '+' in func_address:
                                        offset_str = func_address.split('+', 1)[1]
                                        try:
                                            vaddr = int(offset_str, 16)
                                            call_stack_info = self._get_call_stack_info(
                                                func_file_path, func_address, vaddr
                                            )
                                            if call_stack_info:
                                                func['call_stack_info'] = call_stack_info
                                                logger.debug(
                                                    f'å‡½æ•° {func.get("function_id", "unknown")} é‡æ–°è·å–è°ƒç”¨å †æ ˆä¿¡æ¯æˆåŠŸ: {len(call_stack_info.get("callers", []))} ä¸ªè°ƒç”¨è€…, {len(call_stack_info.get("callees", []))} ä¸ªè¢«è°ƒç”¨è€…'
                                                )
                                        except (ValueError, Exception) as e:
                                            logger.debug(f'æ— æ³•è§£æåœ°å€ {func_address}: {e}')
                                except Exception as e:
                                    logger.debug(f'è·å–è°ƒç”¨å †æ ˆä¿¡æ¯å¤±è´¥: {e}')

                        # ä½¿ç”¨åŸºç¡€ä¸Šä¸‹æ–‡ï¼Œä¸æ·»åŠ è°ƒç”¨å †æ ˆä¿¡æ¯ï¼ˆè°ƒç”¨å †æ ˆä¿¡æ¯ä¼šåœ¨æ¯ä¸ªå‡½æ•°ä¸­å•ç‹¬æ˜¾ç¤ºï¼‰
                        context = base_context

                    # å¯¹è¯¥ SO æ–‡ä»¶ç»„çš„å‡½æ•°è¿›è¡Œæ‰¹é‡åˆ†æ
                    batch_results = self.llm_analyzer.batch_analyze_functions(so_functions, context=context)
                    all_batch_results.extend(batch_results)

                batch_results = all_batch_results

                # ç¬¬ä¸‰æ­¥ï¼šå°† LLM åˆ†æç»“æœåˆå¹¶åˆ° results ä¸­
                logger.info('\nStep 3: Merging LLM analysis results...')
                batch_results_map = {r.get('function_id', ''): r for r in batch_results}
                logger.info(
                    f'æ‰¹é‡åˆ†æè¿”å›äº† {len(batch_results)} ä¸ªç»“æœï¼Œfunction_id: {[r.get("function_id") for r in batch_results]}'
                )
                logger.info(f'results contains {len(results)} functions, rank: {[r.get("rank") for r in results]}')

                missing_results = []
                for result in results:
                    func_id = f'func_{result["rank"]}'
                    if func_id in batch_results_map:
                        batch_result = batch_results_map[func_id]
                        # ç§»é™¤ function_idï¼Œä¿ç•™å…¶ä»–å­—æ®µ
                        llm_result = {k: v for k, v in batch_result.items() if k != 'function_id'}
                        result['llm_result'] = llm_result
                        logger.info(
                            f'âœ… å‡½æ•° #{result["rank"]} ({func_id}, åœ°å€: {result.get("address", "unknown")}) æˆåŠŸåˆå¹¶ LLM ç»“æœ: {llm_result.get("function_name", "None")}, {llm_result.get("functionality", "æœªçŸ¥")}'
                        )
                    else:
                        # å¦‚æœæ‰¹é‡åˆ†æç»“æœä¸­æ²¡æœ‰è¯¥å‡½æ•°ï¼Œä½¿ç”¨é»˜è®¤ç»“æœ
                        missing_results.append(func_id)
                        result['llm_result'] = {
                            'function_name': None,
                            'functionality': 'æœªçŸ¥',
                            'confidence': 'ä½',
                            'reasoning': 'æ‰¹é‡ LLM åˆ†æç»“æœä¸­æœªæ‰¾åˆ°è¯¥å‡½æ•°',
                        }
                        logger.warning(
                            f' å‡½æ•° #{result["rank"]} ({func_id}, åœ°å€: {result.get("address", "unknown")}) åœ¨æ‰¹é‡åˆ†æç»“æœä¸­æœªæ‰¾åˆ°'
                        )

                if missing_results:
                    logger.warning(
                        f' ä»¥ä¸‹ {len(missing_results)} ä¸ªå‡½æ•°åœ¨æ‰¹é‡åˆ†æç»“æœä¸­æœªæ‰¾åˆ°: {", ".join(missing_results)}'
                    )
        else:
            # å•ä¸ªåˆ†ææ¨¡å¼ï¼šé€ä¸ªåˆ†æï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            results = []
            for item in data_list:
                file_path = item['file_path']
                address = item['address']
                call_count = item.get('call_count', 0)
                event_count = item.get('event_count', None)  # ä» Excel è¯»å–æ—¶å¯èƒ½åŒ…å« event_count
                rank = len(results) + 1

                # å¤„ç† HAP åœ°å€ï¼ˆå¦‚æœä¹‹å‰æ²¡æœ‰è§£ææˆåŠŸï¼Œå†æ¬¡å°è¯•ï¼‰
                if HAP_RESOLVER_AVAILABLE and is_hap_address(address) and self.perf_db_file:
                    hap_resolution = item.get('hap_resolution')
                    if not hap_resolution or not hap_resolution.get('resolved'):
                        logger.info(f'ğŸ”„ Re-resolving HAP address: {address}')
                        resolution = resolve_hap_address_from_perfdb(
                            self.perf_db_file, address, quick_mode=True, so_dir=self.so_dir
                        )
                        if resolution and resolution.get('resolved'):
                            file_path = resolution['so_file_path']
                            address = f'{resolution["so_name"]}+0x{resolution["so_offset"]:x}'
                            logger.info(f'  âœ… Resolution successful: {address}')
                        else:
                            logger.warning(f'   Unable to resolve HAP address, skipping: {address}')
                            continue

                result = self.analyze_function(file_path, address, call_count, rank, event_count)
                if result:
                    results.append(result)

                # æ¯10ä¸ªå‡½æ•°æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if len(results) % 10 == 0:
                    logger.info(f'\nProgress: {len(results)}/{top_n} function analysis completed')

        # åˆ†æå®Œæˆåï¼Œä¿å­˜æ‰€æœ‰ç¼“å­˜å’Œç»Ÿè®¡ï¼ˆç»Ÿä¸€åœ¨è¿™é‡Œè°ƒç”¨ï¼Œé¿å…é‡å¤ï¼‰
        if self.use_llm and self.llm_analyzer:
            self.llm_analyzer.finalize()

        return results

    def _analyze_top_functions_sequential(self, top_df, top_n):
        """é€ä¸ªåˆ†æå‰ N ä¸ªå‡½æ•°ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        results = []
        for _idx, row in top_df.iterrows():
            file_path = row['æ–‡ä»¶è·¯å¾„']
            address = row['åœ°å€']
            call_count = row.get('è°ƒç”¨æ¬¡æ•°', 0)

            # è¯»å– event_countï¼Œå¤„ç† NaN å€¼
            event_count = None
            if 'æŒ‡ä»¤æ•°(event_count)' in row:
                event_count_val = row['æŒ‡ä»¤æ•°(event_count)']
                # å¤„ç† pandas çš„ NaN å€¼
                if pd.notna(event_count_val) and event_count_val > 0:
                    event_count = int(event_count_val)

            rank = len(results) + 1

            result = self.analyze_function(file_path, address, call_count, rank, event_count)
            if result:
                results.append(result)

            # æ¯10ä¸ªå‡½æ•°æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if len(results) % 10 == 0:
                logger.info(f'\nProgress: {len(results)}/{top_n} function analysis completed')

        # æ¸…ç† radare2 åˆ†æå™¨ï¼ˆåˆ†æå®Œæˆåé‡Šæ”¾èµ„æºï¼‰
        if hasattr(self, '_r2_analyzers'):
            self.cleanup_r2_analyzers()

        return results

    def cleanup_r2_analyzers(self):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜çš„ radare2 åˆ†æå™¨å®ä¾‹"""
        if not hasattr(self, '_r2_analyzers'):
            return
        for so_file_path, r2_analyzer in self._r2_analyzers.items():
            try:
                r2_analyzer.__exit__(None, None, None)  # æ‰‹åŠ¨é€€å‡ºä¸Šä¸‹æ–‡ç®¡ç†å™¨
            except Exception as e:
                logger.info(f' Failed to close radare2 analyzer ({Path(so_file_path).name}): {e}')
        self._r2_analyzers.clear()
        logger.info('âœ… Cleaned up all radare2 analyzer instances')

    def save_results(
        self,
        results,
        output_file=None,
        html_file=None,
        time_tracker=None,
        top_n=None,
        output_dir=None,
    ):
        """ä¿å­˜åˆ†æç»“æœ

        Args:
            results: åˆ†æç»“æœåˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™æ ¹æ® top_n è‡ªåŠ¨ç”Ÿæˆï¼‰
            html_file: HTML æŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™æ ¹æ® output_file è‡ªåŠ¨ç”Ÿæˆï¼‰
            time_tracker: æ—¶é—´è·Ÿè¸ªå™¨ï¼ˆå¯é€‰ï¼‰
            top_n: åˆ†æçš„æ•°é‡ï¼ˆç”¨äºç”Ÿæˆæ–‡ä»¶åï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç»“æœæ•°é‡æ¨æ–­ï¼‰
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è¾“å‡ºç›®å½•ï¼‰
        """
        output_dir = config.get_output_dir() if output_dir is None else Path(output_dir)
        config.ensure_output_dir(output_dir)

        # å¦‚æœæ²¡æœ‰æä¾› top_nï¼Œä»ç»“æœæ•°é‡æ¨æ–­
        if top_n is None:
            top_n = len(results) if results else DEFAULT_TOP_N

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ event_countï¼ˆå¦‚æœç»“æœä¸­æœ‰ event_count å­—æ®µï¼‰
        use_event_count = any('event_count' in result and result.get('event_count', 0) > 0 for result in results)

        # å‡†å¤‡ Excel æ•°æ®
        excel_data = []
        for result in results:
            llm_result = result.get('llm_result', {})
            # ç¡®ä¿å­—ç¬¦ä¸²å¸¸é‡ä¸ä¸º Noneï¼Œç©ºå­—ç¬¦ä¸²ä¹Ÿè¦ä¿ç•™
            strings_value = result.get('strings', '')
            if strings_value is None:
                strings_value = ''
            elif isinstance(strings_value, list):
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
                strings_value = ', '.join(str(s) for s in strings_value if s)
            elif isinstance(strings_value, str):
                # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨ï¼ˆå¯èƒ½æ˜¯é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ï¼‰
                strings_value = strings_value if strings_value else ''
            else:
                strings_value = str(strings_value) if strings_value else ''

            # å¤„ç†è°ƒç”¨é“¾ä¿¡æ¯ï¼ˆcall_stack_infoï¼‰ï¼Œåˆå¹¶è°ƒç”¨è€…å’Œè¢«è°ƒç”¨è€…ä¿¡æ¯
            call_stack_info = result.get('call_stack_info')
            call_stack_str = ''

            # æ”¶é›†æ‰€æœ‰è°ƒç”¨å…³ç³»
            callers_list = []
            callees_list = []

            # ä» call_stack_info è·å–è°ƒç”¨è€…å’Œè¢«è°ƒç”¨è€…
            if call_stack_info:
                callers = call_stack_info.get('callers', [])
                callees = call_stack_info.get('callees', [])

                if callers:
                    for caller in callers[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ªè°ƒç”¨è€…
                        caller_name = caller.get('symbol_name', '')
                        caller_addr = caller.get('address', '')
                        if caller_name:
                            callers_list.append(f'{caller_name}({caller_addr})')
                        elif caller_addr:
                            callers_list.append(caller_addr)

                if callees:
                    for callee in callees[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ªè¢«è°ƒç”¨è€…
                        callee_name = callee.get('symbol_name', '')
                        callee_addr = callee.get('address', '')
                        if callee_name:
                            callees_list.append(f'{callee_name}({callee_addr})')
                        elif callee_addr:
                            callees_list.append(callee_addr)

            # å¦‚æœæ²¡æœ‰ä» call_stack_info è·å–åˆ°è¢«è°ƒç”¨è€…ï¼Œå°è¯•ä» called_functions è·å–
            if not callees_list:
                called_functions = result.get('called_functions', [])
                if called_functions:
                    callees_list = called_functions[:10]  # æœ€å¤šæ˜¾ç¤º10ä¸ª

            # æ ¼å¼åŒ–è°ƒç”¨é“¾ä¿¡æ¯
            call_stack_parts = []
            if callers_list:
                call_stack_parts.append(f'è°ƒç”¨è€…: {", ".join(callers_list)}')
            if callees_list:
                call_stack_parts.append(f'è¢«è°ƒç”¨: {", ".join(callees_list)}')

            if call_stack_parts:
                call_stack_str = ' | '.join(call_stack_parts)

            # è·å–å‡½æ•°è¾¹ç•Œä¿¡æ¯ï¼ˆç”¨äºåœ°å€åŒ¹é…ï¼‰
            func_info = result.get('func_info', {})
            func_start = func_info.get('minbound', func_info.get('offset', 0))
            func_size = func_info.get('size', 0)
            func_end = func_info.get('maxbound', func_start + func_size) if func_size > 0 else func_start + 2000

            # ç¡®ä¿ llm_result å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
            if 'llm_result' not in result or result.get('llm_result') is None:
                result['llm_result'] = {
                    'function_name': None,
                    'functionality': 'æœªçŸ¥',
                    'confidence': 'ä½',
                    'reasoning': 'LLM åˆ†æç»“æœç¼ºå¤±',
                }

            llm_result = result.get('llm_result', {})

            # å®‰å…¨åœ°è·å– LLM ç»“æœå­—æ®µï¼Œç¡®ä¿ None å€¼è½¬æ¢ä¸ºç©ºå­—ç¬¦ä¸²
            def safe_get_llm_field(field_name, llm_result_dict, default=''):
                value = llm_result_dict.get(field_name) if llm_result_dict else None
                if value is None:
                    return default
                result_value = str(value) if value else default
                # å¦‚æœæ˜¯å‡½æ•°åï¼Œæ·»åŠ  "Function: " å‰ç¼€
                if field_name == 'function_name' and result_value and result_value != default:
                    result_value = f'Function: {result_value}'
                return result_value

            row_data = {
                'æ’å': result['rank'],
                'æ–‡ä»¶è·¯å¾„': result['file_path'],
                'åœ°å€': result['address'],
                'åç§»é‡': result['offset'],
                'SOæ–‡ä»¶': result['so_file'],
                'æŒ‡ä»¤æ•°': result['instruction_count'],
                'å­—ç¬¦ä¸²å¸¸é‡': strings_value,  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œä¸æ˜¯ None
                'è°ƒç”¨é“¾ä¿¡æ¯': call_stack_str,  # åˆå¹¶äº†è°ƒç”¨è€…å’Œè¢«è°ƒç”¨è€…ä¿¡æ¯
                'LLMæ¨æ–­å‡½æ•°å': safe_get_llm_field('function_name', llm_result, ''),
                'LLMåŠŸèƒ½æè¿°': safe_get_llm_field('functionality', llm_result, 'æœªçŸ¥'),
                'è´Ÿè½½é—®é¢˜è¯†åˆ«ä¸ä¼˜åŒ–å»ºè®®': safe_get_llm_field('performance_analysis', llm_result, ''),
                'LLMç½®ä¿¡åº¦': safe_get_llm_field('confidence', llm_result, 'ä½'),
                'LLMæ¨ç†è¿‡ç¨‹': safe_get_llm_field('reasoning', llm_result, ''),
                'å‡½æ•°èµ·å§‹åç§»': f'0x{func_start:x}' if func_start else '',
                'å‡½æ•°å¤§å°': func_size if func_size > 0 else '',
                'å‡½æ•°ç»“æŸåç§»': f'0x{func_end:x}' if func_end > func_start else '',
            }

            # æ ¹æ®ç»Ÿè®¡æ–¹å¼é€‰æ‹©æ˜¾ç¤º event_count æˆ– call_count
            if use_event_count and 'event_count' in result:
                # ä½¿ç”¨ event_count ä½œä¸ºä¸»è¦æŒ‡æ ‡
                row_data['æŒ‡ä»¤æ•°(event_count)'] = result['event_count']
                # åŒæ—¶æ˜¾ç¤º call_countï¼ˆå³ä½¿ä¸º0ä¹Ÿæ˜¾ç¤ºï¼Œç”¨äºå¯¹æ¯”ï¼‰
                row_data['è°ƒç”¨æ¬¡æ•°'] = result.get('call_count', 0)
            else:
                # ä½¿ç”¨ call_count ä½œä¸ºä¸»è¦æŒ‡æ ‡
                row_data['è°ƒç”¨æ¬¡æ•°'] = result.get('call_count', 0)
                # å¦‚æœæœ‰ event_countï¼Œä¹Ÿæ˜¾ç¤ºï¼ˆç”¨äºå¯¹æ¯”ï¼‰
                if 'event_count' in result and result.get('event_count', 0) > 0:
                    row_data['æŒ‡ä»¤æ•°(event_count)'] = result['event_count']

            excel_data.append(row_data)

        # ä¿å­˜ Excel
        df = pd.DataFrame(excel_data)

        # ç¡®ä¿å­—ç¬¦ä¸²å¸¸é‡åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œé¿å…ç©ºå­—ç¬¦ä¸²è¢«è½¬æ¢ä¸º nan
        if 'å­—ç¬¦ä¸²å¸¸é‡' in df.columns:
            df['å­—ç¬¦ä¸²å¸¸é‡'] = df['å­—ç¬¦ä¸²å¸¸é‡'].fillna('').astype(str).replace('nan', '').replace('None', '')

        # ç¡®ä¿è°ƒç”¨é“¾ä¿¡æ¯åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
        if 'è°ƒç”¨é“¾ä¿¡æ¯' in df.columns:
            df['è°ƒç”¨é“¾ä¿¡æ¯'] = df['è°ƒç”¨é“¾ä¿¡æ¯'].fillna('').astype(str).replace('nan', '').replace('None', '')

        # ç¡®ä¿ LLM ç›¸å…³åˆ—ä¸æ˜¯ nanï¼ˆå¤„ç† None å€¼ï¼‰
        llm_columns = ['LLMæ¨æ–­å‡½æ•°å', 'LLMåŠŸèƒ½æè¿°', 'è´Ÿè½½é—®é¢˜è¯†åˆ«ä¸ä¼˜åŒ–å»ºè®®', 'LLMç½®ä¿¡åº¦', 'LLMæ¨ç†è¿‡ç¨‹']
        for col in llm_columns:
            if col in df.columns:
                # å°† None å’Œ nan æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²æˆ–é»˜è®¤å€¼
                if col == 'LLMåŠŸèƒ½æè¿°':
                    default_value = 'æœªçŸ¥'
                elif col == 'è´Ÿè½½é—®é¢˜è¯†åˆ«ä¸ä¼˜åŒ–å»ºè®®':
                    default_value = ''
                elif col == 'LLMç½®ä¿¡åº¦':
                    default_value = 'ä½'
                else:
                    default_value = ''
                df[col] = (
                    df[col]
                    .fillna(default_value)
                    .astype(str)
                    .replace('nan', default_value)
                    .replace('None', default_value)
                )

        # ç¡®ä¿åˆ—çš„é¡ºåºï¼šå¦‚æœä½¿ç”¨ event_countï¼Œå°† event_count åˆ—æ”¾åœ¨è°ƒç”¨æ¬¡æ•°ä¹‹å‰
        if use_event_count and 'æŒ‡ä»¤æ•°(event_count)' in df.columns:
            # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåº
            cols = list(df.columns)
            if 'æŒ‡ä»¤æ•°(event_count)' in cols and 'è°ƒç”¨æ¬¡æ•°' in cols:
                # ç§»é™¤è¿™ä¸¤ä¸ªåˆ—
                cols.remove('æŒ‡ä»¤æ•°(event_count)')
                cols.remove('è°ƒç”¨æ¬¡æ•°')
                # æ‰¾åˆ° 'åç§»é‡' çš„ä½ç½®ï¼Œåœ¨å…¶åæ’å…¥
                if 'åç§»é‡' in cols:
                    idx = cols.index('åç§»é‡')
                    cols.insert(idx + 1, 'æŒ‡ä»¤æ•°(event_count)')
                    cols.insert(idx + 2, 'è°ƒç”¨æ¬¡æ•°')
                # å¦‚æœæ²¡æœ‰åç§»é‡ï¼Œåœ¨åœ°å€åæ’å…¥
                elif 'åœ°å€' in cols:
                    idx = cols.index('åœ°å€')
                    cols.insert(idx + 1, 'æŒ‡ä»¤æ•°(event_count)')
                    cols.insert(idx + 2, 'è°ƒç”¨æ¬¡æ•°')
                else:
                    cols.insert(0, 'æŒ‡ä»¤æ•°(event_count)')
                    cols.insert(1, 'è°ƒç”¨æ¬¡æ•°')
                df = df[cols]

        if output_file is None:
            excel_file = output_dir / CALL_COUNT_ANALYSIS_PATTERN.format(n=top_n)
        else:
            excel_file = Path(output_file) if isinstance(output_file, str) else output_file
            if not excel_file.is_absolute():
                output_file_str = str(output_file)
                output_dir_str = str(output_dir)
                excel_file = Path(output_file) if output_dir_str in output_file_str else output_dir / excel_file

        # ä½¿ç”¨ openpyxl è®¾ç½®æ ¼å¼
        if not all([Workbook, Alignment, get_column_letter]):
            raise ImportError('openpyxl æœªå®‰è£…ï¼Œè¯·æ‰§è¡Œ pip install openpyxl')

        wb = Workbook()
        ws = wb.active
        ws.title = 'ç¼ºå¤±ç¬¦å·å‡½æ•°åˆ†æ'

        headers = list(df.columns)
        ws.append(headers)

        for _, row in df.iterrows():
            ws.append([row[col] for col in headers])

        # è®¾ç½®åˆ—å®½
        column_widths = {
            'æ’å': 8,
            'æ–‡ä»¶è·¯å¾„': 60,
            'åœ°å€': 40,
            'åç§»é‡': 15,
            'è°ƒç”¨æ¬¡æ•°': 12,
            'æŒ‡ä»¤æ•°(event_count)': 18,
            'SOæ–‡ä»¶': 50,
            'æŒ‡ä»¤æ•°': 10,
            'å­—ç¬¦ä¸²å¸¸é‡': 40,
            'è°ƒç”¨é“¾ä¿¡æ¯': 60,  # åˆå¹¶äº†è°ƒç”¨è€…å’Œè¢«è°ƒç”¨è€…ä¿¡æ¯
            'LLMæ¨æ–­å‡½æ•°å': 30,
            'LLMåŠŸèƒ½æè¿°': 80,
            'è´Ÿè½½é—®é¢˜è¯†åˆ«ä¸ä¼˜åŒ–å»ºè®®': 100,
            'LLMç½®ä¿¡åº¦': 12,
            'LLMæ¨ç†è¿‡ç¨‹': 80,
            'å‡½æ•°èµ·å§‹åç§»': 15,
            'å‡½æ•°å¤§å°': 12,
            'å‡½æ•°ç»“æŸåç§»': 15,
        }

        for col_idx, header in enumerate(headers, 1):
            col_letter = get_column_letter(col_idx)
            ws.column_dimensions[col_letter].width = column_widths.get(header, 15)

            # è®¾ç½®æ–‡æœ¬æ¢è¡Œ
            if header in [
                'LLMåŠŸèƒ½æè¿°',
                'è´Ÿè½½é—®é¢˜è¯†åˆ«ä¸ä¼˜åŒ–å»ºè®®',
                'LLMæ¨ç†è¿‡ç¨‹',
                'æ–‡ä»¶è·¯å¾„',
                'å­—ç¬¦ä¸²å¸¸é‡',
                'è°ƒç”¨é“¾ä¿¡æ¯',
            ]:
                for row_idx in range(2, len(df) + 2):
                    cell = ws[f'{col_letter}{row_idx}']
                    cell.alignment = Alignment(wrap_text=True, vertical='top')

        wb.save(excel_file)
        logger.info(f'\nâœ… Excel report generated: {excel_file}')

        # ä¸å†ç”Ÿæˆå•ç‹¬çš„ HTML æŠ¥å‘Šæ–‡ä»¶
        # HTML æŠ¥å‘Šå†…å®¹å°†åœ¨ Step 4 ä¸­ç›´æ¥åµŒå…¥åˆ° hiperf_report.html ä¸­
        logger.info('â„¹ï¸  Skipping HTML report file generation (will be embedded in hiperf_report.html in Step 4)')

        return excel_file
